{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import regionmask\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import cartopy.feature as cfeat\n",
    "import matplotlib.patches as mpatches\n",
    "import datetime\n",
    "\n",
    "import xesmf as xe\n",
    "\n",
    "import geopandas\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "import feather\n",
    "\n",
    "import glob\n",
    "\n",
    "import calendar\n",
    "\n",
    "from shapely.geometry import Point, Polygon\n",
    "from shapely.ops import nearest_points\n",
    "from math import radians, cos, sin, asin, sqrt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import plotting\n",
    "\n",
    "np.seterr(invalid='ignore'); # disable a warning from matplotlib and cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep O3 and PM2.5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##module load cdo\n",
    "##cd Outputdir\n",
    "##mkdir ../merged_data\n",
    "##create specific file: \n",
    "###### for file in GEOSChem.SpeciesConc.2016*; do cdo -selvar,SpeciesConc_O3 $file O3_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime O3*2016$month*.nc4 ../merged_data/merged_O3_$month.nc ; done\n",
    "###### for file in GEOSChem.AerosolMass.2016*; do cdo -selvar,PM25 $file PM_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime PM*2016$month*.nc4 ../merged_data/merged_PM_$month.nc ; done\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Daily Mean Winter file for each simulation and a Summer Hourly File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = ['nei_NA','normal_NA','nonuc_NA','nonuc_coal_NA','egrid_NA','epa_NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for poll in ['merged_O3']:\n",
    "    for path in all_paths:\n",
    "        ds6 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_06_hourly.nc')\n",
    "        ds7 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_07_hourly.nc')\n",
    "        ds8 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_08_hourly.nc')\n",
    "        for idx, ds in zip(['06','07','08'],[ds6, ds7, ds8]):\n",
    "            ds = ds.groupby('time.date').mean(dim = 'time')\n",
    "            ds['date'] = pd.to_datetime(ds['date'])\n",
    "            ds = ds.rename({'date':'time'})\n",
    "            ds.to_netcdf(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_daily_mean_{idx}.nc')\n",
    "            print(poll, path, idx, 'done')\n",
    "        ds_JJA = xr.merge([ds6, ds7, ds8])\n",
    "        #save hourly mean DS\n",
    "        ds_JJA.to_netcdf(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_JJA_hourly.nc4')\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "for poll in ['merged_PM']:\n",
    "    for path in ['nei_NA','normal_NA','nonuc_NA','egrid_NA','epa_NA']:\n",
    "        ds6 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_06_hourly.nc')\n",
    "        ds7 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_07_hourly.nc')\n",
    "        ds8 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_08_hourly.nc')\n",
    "        for idx, ds in zip(['06','07','08'],[ds6, ds7, ds8]):\n",
    "            ds = ds.groupby('time.date').mean(dim = 'time')\n",
    "            ds['date'] = pd.to_datetime(ds['date'])\n",
    "            ds = ds.rename({'date':'time'})\n",
    "            ds.to_netcdf(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_daily_mean_{idx}.nc')\n",
    "            print(poll, path, idx, 'done')\n",
    "        ds_JJA = xr.merge([ds6, ds7, ds8])\n",
    "        #save hourly mean DS\n",
    "        ds_JJA.to_netcdf(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_JJA_hourly.nc4')\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine all daily mean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['merged_PM'] = {}\n",
    "ds['merged_O3'] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for poll in ['merged_O3','merged_PM']:\n",
    "    for path in all_paths:\n",
    "        dsb = xr.open_mfdataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_daily_mean_*.nc', combine = 'by_coords')\n",
    "        #dsb = dsb.rename({'date':'time'})\n",
    "        print(poll,path)\n",
    "        ds[poll][path] = dsb.groupby('time.date').mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = utils.combine_and_convert_PM_O3([ds for ds in list(ds['merged_O3'].values())], \n",
    "                    [ds for ds in list(ds['merged_PM'].values())],\n",
    "                    [nm for nm in ds['merged_O3'].keys()],\n",
    "                    [nm for nm in ds['merged_PM'].keys()], 'all_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = ds.rename({'date':'time'})\n",
    "ds['time'] = pd.to_datetime(ds['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['O3'].attrs = {'units':'ppb'}\n",
    "ds['PM25'].attrs = {'units':r'$u$g/m3'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds.to_netcdf('final_data/ds_PM_O3_daily.nc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep NOx, SO2, VOC data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##module load cdo\n",
    "##create specific file and merge each of them: \n",
    "#for file in GEOSChem.SpeciesConc.2016*; do cdo -selvar,SpeciesConc_CH2O $file CH2O_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime CH2O*2016$month*.nc4 ../merged_data/merged_CH2O_$month.nc ; done\n",
    "# && for file in GEOSChem.SpeciesConc.2016*; do cdo -selvar,SpeciesConc_NO $file NO_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime NO*2016$month*.nc4 ../merged_data/merged_NO_$month.nc ; done\n",
    "# && for file in GEOSChem.SpeciesConc.2016*; do cdo -selvar,SpeciesConc_NO2 $file NO2_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime NO2*2016$month*.nc4 ../merged_data/merged_NO2_$month.nc ; done\n",
    "# && for file in GEOSChem.SpeciesConc.2016*; do cdo -selvar,SpeciesConc_SO2 $file SO2_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime SO2*2016$month*.nc4 ../merged_data/merged_SO2_$month.nc ; done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['merged_NO'] = {}\n",
    "ds['merged_NO2'] = {}\n",
    "ds['merged_CH2O'] = {}\n",
    "ds['merged_SO2'] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poll in ['merged_NO','merged_NO2','merged_CH2O','merged_SO2']:\n",
    "    for path in all_paths:\n",
    "        ds[poll][path] = xr.open_mfdataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_*.nc', combine = 'by_coords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gas = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for gas in ['NO2','NO','SO2','CH2O']:\n",
    "    ds_gas[gas] = utils.convert_gases([ds for ds in list(ds[f'merged_{gas}'].values())], [nm for nm in ds[f'merged_{gas}'].keys()], gas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out = xr.merge([ds_gas[gas] for gas in ['NO2','NO','SO2','CH2O']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out = ds_out.groupby('time.date').mean(dim = 'time').rename({'date':'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out['time'] = pd.to_datetime(ds_out['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum our NO2 and NO to get NOx\n",
    "ds_out['NOx'] = (ds_out['NO'] + ds_out['NO2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_out.to_netcdf('final_data/ds_NOX_SO2_CH2O_daily.nc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep HEMCO Emissions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ds_MODEL = xr.open_mfdataset('/net/fs11/d0/emfreese/GCrundirs/nuclearproj/normal_NA/OutputDir/HEMCO_diagnostics.2016*.nc', combine = 'by_coords')\n",
    "ds_egrid = xr.open_mfdataset('/net/fs11/d0/emfreese/GCrundirs/nuclearproj/egrid_NA/OutputDir/HEMCO_diagnostics.2016*.nc', combine = 'by_coords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ds_MODEL = ds_MODEL.isel(lev = 0)\n",
    "ds_egrid = ds_egrid.isel(lev = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ds = xr.concat([ds_egrid, ds_NEI, ds_MODEL, ds_epa], pd.Index(['egrid', 'NEI', 'MODEL','epa'], name='model_name'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ds.to_netcdf('final_data/HEMCO_emis.nc4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-EGO Generation and Emissions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of Plant type (in 2016 EPA egrid TSD section 5)\n",
    "\n",
    "The fuel category for the primary fuel of the plant. This field is “COAL” if the plant’s primary fuel is derived from coal (fuel type = BIT, COG, LIG, RC, SGC, SUB, WC), “OIL” if it is derived from oil (DFO, JF, KER, PC, RFO, WO), “GAS” if it is derived from gas (BU, NG, PG), “OFSL” if it is another fossil fuel (BFG, OG, TDF), “NUCLEAR” if it is derived from nuclear (NUC), “HYDRO” if it is derived from hydro power (WAT), “SOLAR” if it is derived from solar power, (SUN), “WIND” if it is derived from wind power (WND), “GEOTHERMAL” if it is derived from geothermal power (GEO), “OTHF” if it is derived from waste heat/unknown/purchased (MWH, OTH, PRG, PUR, WH), and “BIOMASS” if it is derived from biomass sources (AB, BLQ, LFG, MSW, OBG, OBL, OBS, SLW, WDL, WDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##### import files and change hydro #####\n",
    "\n",
    "###get ORIS, plant type, and Region Name from our modified generation file\n",
    "oris_nonuc_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_no-nuclear_all-generators_20k-new_name.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "oris_normal_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_normal.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "oris_nocoal_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_no-nuclear_no_coal.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "\n",
    "oris_nonuc_df.loc[oris_nonuc_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "oris_normal_df.loc[oris_normal_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "oris_nocoal_df.loc[oris_nocoal_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "\n",
    "###import egrid generation data and modify the index to be ORISCode\n",
    "egrid_df_raw = pd.read_excel('../raw_data/egrid2016_data.xlsx',sheet_name='GEN16', usecols='D, J, L')\n",
    "egrid = egrid_df_raw.drop(egrid_df_raw.index[0]).rename(columns={'DOE/EIA ORIS plant or facility code':'ORISCode'})\n",
    "#change those with no capacity to 1 (so that our capacity factor calculation doesn't fail)\n",
    "egrid.loc[egrid['Generator nameplate capacity (MW)'] == 0, 'Generator nameplate capacity (MW)'] = 1\n",
    "#calculate the capacity factor of generation in egrid\n",
    "egrid['egrid_capafactor'] = egrid['Generator annual net generation (MWh)'] / (8760 * egrid['Generator nameplate capacity (MW)']) \n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_normal_df=feather.read_dataframe('../optimization_model/outputs/gen_normal.feather')\n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_nonuc_df=feather.read_dataframe('../optimization_model/outputs/gen_no-nuclear.feather')\n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_nocoal_df=feather.read_dataframe('../optimization_model/outputs/gen_no-nuclear_no-coal.feather')\n",
    "\n",
    "\n",
    "### import the plant type from our egrid dataset\n",
    "planttype_df_raw=pd.read_excel('../raw_data/egrid2016_data.xlsx',sheet_name='PLNT16', usecols='D, W')\n",
    "planttype_df=planttype_df_raw.drop(egrid_df_raw.index[0]).rename(columns={'DOE/EIA ORIS plant or facility code':'ORISCode', \n",
    "                                                                       'Plant primary coal/oil/gas/ other fossil fuel category':'planttype'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### change index and convert to xarray #####\n",
    "\n",
    "#group by ORIS code and make a date and ORIS code multi index\n",
    "gen_nonuc_df = pd.concat([gen_nonuc_df,oris_nonuc_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "gen_normal_df = pd.concat([gen_normal_df,oris_normal_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "gen_nocoal_df = pd.concat([gen_nocoal_df,oris_nocoal_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "\n",
    "gen_normal_df = gen_normal_df.stack()\n",
    "gen_nonuc_df = gen_nonuc_df.stack()\n",
    "gen_nocoal_df = gen_nocoal_df.stack()\n",
    "\n",
    "gen_nonuc_df.index.names = (['ORISCode','date'])\n",
    "gen_normal_df.index.names = (['ORISCode','date'])\n",
    "gen_nocoal_df.index.names = (['ORISCode','date'])\n",
    "\n",
    "\n",
    "egrid = egrid.groupby('ORISCode').sum().drop(columns = 'egrid_capafactor') #drop capacity factor because that isn't the sum\n",
    " \n",
    "planttype=planttype_df.set_index(['ORISCode'])\n",
    "planttype_sort=planttype.sort_values(by=\"ORISCode\")\n",
    "\n",
    "egrid_planttype_df = pd.concat((egrid,planttype_sort),axis=1)\n",
    "\n",
    "#convert to xarray datasets\n",
    "oris_nonuc_ds = oris_nonuc_df.to_xarray()\n",
    "oris_normal_ds = oris_normal_df.to_xarray()\n",
    "oris_nocoal_ds = oris_nocoal_df.to_xarray()\n",
    "\n",
    "egrid_planttype_ds = egrid_planttype_df.to_xarray()\n",
    "\n",
    "gen_normal_ds = gen_normal_df.to_xarray()\n",
    "gen_nonuc_ds = gen_nonuc_df.to_xarray()\n",
    "gen_nocoal_ds = gen_nocoal_df.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### combine our egrid and generation info for no nuclear and normal models ####\n",
    "nonuc_gmodel_egrid_ds = utils.combine_egrid_generation(oris_nonuc_ds, gen_nonuc_ds, egrid_planttype_ds)\n",
    "normal_gmodel_egrid_ds = utils.combine_egrid_generation(oris_normal_ds, gen_normal_ds, egrid_planttype_ds)\n",
    "nocoal_gmodel_egrid_ds = utils.combine_egrid_generation(oris_nocoal_ds, gen_nocoal_ds, egrid_planttype_ds)\n",
    "\n",
    "gmodel_egrid_raw_ds = xr.concat([nonuc_gmodel_egrid_ds, normal_gmodel_egrid_ds,nocoal_gmodel_egrid_ds], \n",
    "                                pd.Index(['nonuc_model','normal_model','nonuc_nocoal_model'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fix datetime and add dec 31st at the 23rd hour ####\n",
    "\n",
    "###create a dict for dec 31st 23rd hour (just copy the dec 31 22nd hour data over)\n",
    "dec23_ds = gmodel_egrid_raw_ds.copy().isel(date = [-1])\n",
    "#change date to datetime for the 23rd hour\n",
    "dec23_ds['date'] = [datetime.datetime(2017,12,31,23,0)]\n",
    "\n",
    "###change all dates to datetime\n",
    "#make a list of dates for the year\n",
    "base = datetime.datetime(2017, 1, 1) #base date\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(8759)] #loop through all hours but the final one (we will add it in above)\n",
    "date_list\n",
    "\n",
    "gmodel_egrid_raw_ds['date'] = date_list\n",
    "gmodel_egrid_ds = xr.merge([gmodel_egrid_raw_ds, dec23_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "To convert our generation in MWh to get the emissions in kg/sec, we use the following (and making sure our emissions rates are in kg/sec)\n",
    "\n",
    "$\\text{Mwh}/3600sec -> (\\text{Mw}/s) * kg/\\text{Mw} -> kg/s$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### add NOx, SO2, CO2, CH4 emissions to gmodel_egrid dataset ######\n",
    "\n",
    "no_mult = 0.8544304 # NO/NOx as estimated from NEI2011 inventory\n",
    "no2_mult = 1 - 0.8544304 # NO2/NOx as estimated from NEI2011 inventory\n",
    "\n",
    "### process our emissions by multiplying generation* emissions factors\n",
    "gmodel_egrid_ds['model_NO_rate']  = no_mult * gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['model_NO2_rate']  = no2_mult * gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['model_SO2_rate']  =  gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLSO2RTA']\n",
    "gmodel_egrid_ds['model_CO2_rate']  =  gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLCO2RTA']\n",
    "gmodel_egrid_ds['model_CH4_rate']  =  gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLCH4RTA']\n",
    "\n",
    "gmodel_egrid_ds['egrid_annual_NO_rate']  = no_mult * gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['egrid_annual_NO2_rate']  = no2_mult * gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['egrid_annual_SO2_rate']  =  gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLSO2RTA']\n",
    "gmodel_egrid_ds['egrid_annual_CO2_rate']  =  gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLCO2RTA']\n",
    "gmodel_egrid_ds['egrid_annual_CH4_rate']  =  gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLCH4RTA']\n",
    "\n",
    "##### annual generation in original dataset ######\n",
    "gmodel_egrid_ds['annual_modelgeneration'] = gmodel_egrid_ds['modelgeneration'].groupby('ORISCode').sum(dim = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### create new datasets for region and fuel type grouped data, add annual generation to normal ds ######\n",
    "\n",
    "###list of pollutants\n",
    "pollutants = ['NO','NO2','SO2','CO2','CH4']\n",
    "\n",
    "def create_grouped_ds(grouper, model):\n",
    "    grouped_ds = gmodel_egrid_ds.copy().sel(model_name = model).groupby(grouper).sum()\n",
    "    grouped_ds = grouped_ds.drop(['model_capafactor','PLCH4RTA','PLCO2RTA','PLNOXRTA','PLSO2RTA','PLN2ORTA'])\n",
    "    return(grouped_ds)\n",
    "\n",
    "###create datasets of grouped fuel type and region name, combine\n",
    "nonuc_fueltype_grouped_ds = create_grouped_ds('fueltype', 'nonuc_model')\n",
    "normal_fueltype_grouped_ds = create_grouped_ds('fueltype', 'normal_model')\n",
    "nocoal_fueltype_grouped_ds = create_grouped_ds('fueltype', 'nonuc_nocoal_model')\n",
    "\n",
    "nonuc_regionname_grouped_ds = create_grouped_ds('regionname', 'nonuc_model')\n",
    "normal_regionname_grouped_ds = create_grouped_ds('regionname', 'normal_model')\n",
    "nocoal_regionname_grouped_ds = create_grouped_ds('regionname', 'nonuc_nocoal_model')\n",
    "\n",
    "fueltype_grouped_ds = xr.concat([nonuc_fueltype_grouped_ds, normal_fueltype_grouped_ds,nocoal_fueltype_grouped_ds], \n",
    "                                pd.Index(['nonuc_model','normal_model','nonuc_nocoal_model'], name='model_name'))\n",
    "regionname_grouped_ds = xr.concat([nonuc_regionname_grouped_ds, normal_regionname_grouped_ds, nocoal_regionname_grouped_ds], \n",
    "                                  pd.Index(['nonuc_model','normal_model','nonuc_nocoal_model'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add annual emissions of each pollutant\n",
    "for pollutant in pollutants:\n",
    "    regionname_grouped_ds[f'model_annual_{pollutant}_conc'] = (regionname_grouped_ds[f'model_{pollutant}_rate']*3600).sum(dim = 'date')\n",
    "    fueltype_grouped_ds[f'model_annual_{pollutant}_conc'] = (fueltype_grouped_ds[f'model_{pollutant}_rate']*3600).sum(dim = 'date')\n",
    "    gmodel_egrid_ds[f'model_annual_{pollutant}_conc'] = (gmodel_egrid_ds[f'model_{pollutant}_rate']*3600).sum(dim = 'date')\n",
    "\n",
    "    regionname_grouped_ds[f'egrid_annual_{pollutant}_conc'] = (regionname_grouped_ds[f'egrid_annual_{pollutant}_rate']*3600)\n",
    "    fueltype_grouped_ds[f'egrid_annual_{pollutant}_conc'] = (fueltype_grouped_ds[f'egrid_annual_{pollutant}_rate']*3600)\n",
    "    gmodel_egrid_ds[f'egrid_annual_{pollutant}_conc'] = (gmodel_egrid_ds[f'egrid_annual_{pollutant}_rate']*3600)\n",
    "\n",
    "#add ratio of difference in emissions normal-nonuc/ difference in generation normal-nonuc to the nonuc dataset\n",
    "for pollutant in pollutants:\n",
    "    fueltype_grouped_ds.sel(model_name = 'nonuc_model')[f'normal-nonuc_{pollutant}-gen_ratio'] = (fueltype_grouped_ds.sel(model_name = 'normal_model')[f'model_annual_{pollutant}_conc']-\n",
    "                                                                       fueltype_grouped_ds.sel(model_name = 'nonuc_model')[f'model_annual_{pollutant}_conc'])/(fueltype_grouped_ds.sel(model_name = 'normal_model')['annual_modelgeneration']-\n",
    "                                                                                                                             fueltype_grouped_ds.sel(model_name = 'nonuc_model')['annual_modelgeneration'])\n",
    "    regionname_grouped_ds.sel(model_name = 'nonuc_model')[f'normal-nonuc_{pollutant}-gen_ratio'] = (regionname_grouped_ds.sel(model_name = 'normal_model')[f'model_annual_{pollutant}_conc']-\n",
    "                                                                         regionname_grouped_ds.sel(model_name = 'nonuc_model')[f'model_annual_{pollutant}_conc'])/(regionname_grouped_ds.sel(model_name = 'normal_model')['annual_modelgeneration']-\n",
    "                                                                                                                                 regionname_grouped_ds.sel(model_name = 'nonuc_model')['annual_modelgeneration'])\n",
    "\n",
    "    fueltype_grouped_ds.sel(model_name = 'nonuc_nocoal_model')[f'normal-nonuc_nocoal_{pollutant}-gen_ratio'] = (fueltype_grouped_ds.sel(model_name = 'normal_model')[f'model_annual_{pollutant}_conc']-\n",
    "                                                                       fueltype_grouped_ds.sel(model_name = 'nonuc_model')[f'model_annual_{pollutant}_conc'])/(fueltype_grouped_ds.sel(model_name = 'normal_model')['annual_modelgeneration']-\n",
    "                                                                                                                             fueltype_grouped_ds.sel(model_name = 'nonuc_nocoal_model')['annual_modelgeneration'])\n",
    "    regionname_grouped_ds.sel(model_name = 'nonuc_nocoal_model')[f'normal-nonuc_nocoal_{pollutant}-gen_ratio'] = (regionname_grouped_ds.sel(model_name = 'normal_model')[f'model_annual_{pollutant}_conc']-\n",
    "                                                                         regionname_grouped_ds.sel(model_name = 'nonuc_model')[f'model_annual_{pollutant}_conc'])/(regionname_grouped_ds.sel(model_name = 'normal_model')['annual_modelgeneration']-\n",
    "                                                                                                                                 regionname_grouped_ds.sel(model_name = 'nonuc_nocoal_model')['annual_modelgeneration'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add attributes ####\n",
    "for pollutant in pollutants:\n",
    "    if pollutant == 'NO2' or 'NO':\n",
    "                gmodel_egrid_ds[f'PLNOXRTA'].attrs['units'] = 'kg/Mwh'\n",
    "    else:\n",
    "        gmodel_egrid_ds[f'PL{pollutant}RTA'].attrs['units'] = 'kg/Mwh'\n",
    "    for idx, ds in enumerate([regionname_grouped_ds, fueltype_grouped_ds, gmodel_egrid_ds]):\n",
    "        ds[f'model_{pollutant}_rate'].attrs['units'] = 'kg/s'\n",
    "        ds[f'modelgeneration'].attrs['units'] = 'Mwh'\n",
    "        for typem in ['egrid','model']:\n",
    "            ds[f'annual_{typem}generation'].attrs['units'] = 'Mwh'\n",
    "            ds[f'{typem}_annual_{pollutant}_conc'].attrs['units'] = 'kg'\n",
    "        ds[f'egrid_annual_{pollutant}_rate'].attrs['units'] = 'kg/s'\n",
    "        ds.attrs['group'] = ['Region','Fuel Type','All'][idx]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save datasets #####\n",
    "### Final datasets to save are: gmodel_egrid, fueltype_grouped_ds, regionname_grouped_ds\n",
    "xr.Dataset.to_zarr(regionname_grouped_ds, './final_data/regionname_grouped_emissions_ds.zarr', mode = 'w') #save the dataset \n",
    "xr.Dataset.to_zarr(fueltype_grouped_ds, './final_data/fueltype_grouped_emissions_ds.zarr', mode = 'w') #save the dataset \n",
    "xr.Dataset.to_zarr(gmodel_egrid_ds, './final_data/gmodel_egrid_emissions_ds.zarr', mode = 'w') #save the dataset \n",
    "### datasets partway to save are:\n",
    "xr.Dataset.to_zarr(oris_nonuc_ds, './final_data/oris_nonuc_ds.zarr', mode = 'w') #save the dataset \n",
    "xr.Dataset.to_zarr(oris_nocoal_ds, './final_data/oris_nonuc_coal_ds.zarr', mode = 'w') #save the dataset \n",
    "xr.Dataset.to_zarr(oris_normal_ds, './final_data/oris_normal_ds.zarr', mode = 'w') #save the dataset \n",
    "xr.Dataset.to_zarr(egrid_planttype_ds, './final_data/egrid_ds.zarr', mode = 'w') #save the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERRA2 RH and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import MERRA2 datasets for RH and T #####\n",
    "\n",
    "#import \n",
    "T_ds = xr.open_mfdataset('../../MERRA2/2016/GEOS_0.5x0.625_NA/MERRA2/2016/*/MERRA2.2016*.A1.05x0625.NA.nc4', combine = 'by_coords')\n",
    "RH_ds = xr.open_mfdataset('../../MERRA2/2016/GEOS_0.5x0.625_NA/MERRA2/2016/*/MERRA2.2016*.A3dyn.05x0625.NA.nc4', combine = 'by_coords')\n",
    "\n",
    "#reduce datasets just to T and RH\n",
    "T_ds = T_ds['TS']\n",
    "RH_ds = RH_ds['RH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Select T and RH in the bounds of our US lat and lon #####\n",
    "T = {}\n",
    "RH = {}\n",
    "for region in utils.lat_lon_dict.keys():\n",
    "    T[region] = {}\n",
    "    T[region] = T_ds.sel(\n",
    "        lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "        lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])\n",
    "    ).groupby('time.season').mean()\n",
    "    RH[region] = {}\n",
    "    RH[region] = RH_ds.sel(\n",
    "        lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "        lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])\n",
    "    ).groupby('time.season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate seasonal mean RH and T by region #####\n",
    "T_seasonal_mean = {}\n",
    "RH_seasonal_mean = {}\n",
    "for region in utils.lat_lon_dict.keys():\n",
    "    T_seasonal_mean[region] = {}\n",
    "    RH_seasonal_mean[region] = {}\n",
    "    for seasons in ['DJF','JJA','MAM','SON']:\n",
    "        T_seasonal_mean[region][seasons] = T[region].sel(season = seasons).mean().values\n",
    "        RH_seasonal_mean[region][seasons] = RH[region].sel(season = seasons).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### convert to dataframes ##### \n",
    "\n",
    "T_df = pd.DataFrame.from_dict({(i,j): T_seasonal_mean[i][j]\n",
    "                            for i in T_seasonal_mean.keys() \n",
    "                            for j in T_seasonal_mean[i].keys()},\n",
    "                            orient='index', columns = ['T'])\n",
    "T_df.index = pd.MultiIndex.from_tuples(T_df.index)\n",
    "\n",
    "RH_df = pd.DataFrame.from_dict({(i,j): RH_seasonal_mean[i][j]\n",
    "                            for i in RH_seasonal_mean.keys() \n",
    "                            for j in RH_seasonal_mean[i].keys()},\n",
    "                            orient='index', columns = ['RH'])\n",
    "RH_df.index = pd.MultiIndex.from_tuples(RH_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### merge dataframes #####\n",
    "RH_T_df = pd.merge(\n",
    "    T_df, RH_df, \n",
    "    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save dataframe #####\n",
    "pd.DataFrame.to_csv(RH_T_df, './data/RH_T.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observational Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPA Observational data\n",
    "data is from https://aqs.epa.gov/aqsweb/airdata/download_files.html\n",
    "for the year 2016\n",
    "choosing O3, NO, SO2, PM25 (FEM/FRM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##module load cdo\n",
    "##create specific file and merge each of them: \n",
    "#for file in GEOSChem.AerosolMass.2016*; do cdo -selvar,AerMassNIT $file NIT_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime NIT*2016$month*.nc4 ../merged_data/merged_NIT_$month.nc ; done && for file in GEOSChem.SpeciesConc.2016*; do cdo -selvar,SpeciesConc_SO4 $file SO4_$file; done && for month in {01,02,03,04,05,06,07,08,09,10,11,12}; do cdo mergetime SO4*2016$month*.nc4 ../merged_data/merged_SO4_$month.nc ; done\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}\n",
    "ds['merged_NIT'] = {}\n",
    "ds['merged_SO4'] = {}\n",
    "for poll in ['merged_NIT','merged_SO4']:\n",
    "    for path in all_paths:\n",
    "        ds[poll][path] = xr.open_mfdataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_*.nc', combine = 'by_coords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_gas = {}\n",
    "ds_gas['SO4'] = utils.convert_gases([ds for ds in list(ds[f'merged_SO4'].values())], [nm for nm in ds[f'merged_SO4'].keys()], 'SO4')\n",
    "ds_gas['NIT'] = utils.convert_aerosol([ds for ds in list(ds[f'merged_NIT'].values())], [nm for nm in ds[f'merged_NIT'].keys()], 'NIT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ds_out = xr.merge([ds_gas[gas] for gas in ['NIT','SO4']])\n",
    "ds_out = ds_out.groupby('time.date').mean(dim = 'time').rename({'date':'time'})\n",
    "ds_out['time'] = pd.to_datetime(ds_out['time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2 = xr.open_dataset('final_data/ds_NOX_SO2_CH2O_daily.nc4')\n",
    "ds3 = xr.open_dataset('final_data/ds_PM_O3_daily.nc4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_ds = ds_out.merge(ds2)\n",
    "poll_ds = poll_ds.merge(ds3.drop('lev_bnds'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_obs_df = utils.import_and_edit_EPAobs('../../GEOS_CHEM/obs_data/daily*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVE Observational data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mw_dict = {'ammNO3': 80.043,\n",
    "           'ammSO4': 132.14} #g/mol \n",
    "g_ug = 1e6\n",
    "\n",
    "for species in mw_dict.keys():\n",
    "    ammon_df[f'{species}f:Value'] *= mw_dict[species] * g_ug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_PM.txt', 'PM25', 'MF')\n",
    "s_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_Sulfate.txt', 'SO4', 'SO4f')\n",
    "n_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_Nitrate.txt', 'NIT', 'NO3f')\n",
    "ammon_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_ammonia.txt', 'NH4', 'NH4f')\n",
    "oc_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_OC.txt', 'OC', 'ECf')\n",
    "\n",
    "IMPROVE_df = pd.concat([pm_df, s_df, n_df,oc_df, ammon_df], axis = 0) #concatenate all dataframes and reset the index\n",
    "IMPROVE_df['Date'] = pd.to_datetime(IMPROVE_df['Date']) #change to datetime\n",
    "IMPROVE_df = IMPROVE_df.loc[IMPROVE_df['Arithmetic Mean'] >= 0] #get rid of -999 readings where there is no data\n",
    "IMPROVE_df = IMPROVE_df.loc[(IMPROVE_df['Latitude'].between(24,50,inclusive = True)) & (IMPROVE_df['Longitude'].between(-130,-60,inclusive = True))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists/Dicts of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Lat and Lon of the nested grid US\n",
    "levels_dict = {'PM25':np.arange(0., 20., .5), 'SO2':np.arange(0., 5., .1), \n",
    "               'NO2':np.arange(0., 5., .1), 'NOx':np.arange(0., 5., .1), 'O3':np.arange(0., 70., 1.),\n",
    "               'dif':np.arange(-.3, .31, .01), 'regional_dif':np.arange(-1.5, 1.51, .01)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate data to get GC run data at observational points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPA Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### interpolate data for EPA\n",
    "interp_EPA_df = pd.DataFrame(columns=['Arithmetic Mean', 'Longitude', 'Latitude','model_name','species','date'])\n",
    "\n",
    "for model in ['egrid_NA', 'nei_NA', 'epa_NA', 'normal_NA']:\n",
    "    print(model, end = ', ')\n",
    "    for species in ['PM25', 'SO2', 'NO2', 'O3']:\n",
    "        print(species, end = ', ')\n",
    "        for month in np.arange(1,13):\n",
    "            print(month, end = ', ')\n",
    "            #data selected for date\n",
    "            data = poll_ds.sel(model_name = model)[f'{species}'].groupby('time.month').mean().sel(month = month)\n",
    "            \n",
    "            #new lat and lon in radians\n",
    "            lats_new = EPA_obs_df.loc[(EPA_obs_df['species'] == species)]['Latitude'].unique()\n",
    "            lons_new = EPA_obs_df.loc[(EPA_obs_df['species'] == species)]['Longitude'].unique()\n",
    "            \n",
    "            #interpolation function\n",
    "            interp_data = []\n",
    "            for idx in range(lats_new.size):\n",
    "                interp_data.append(data.sel(lat=lats_new[idx], lon=lons_new[idx], method='nearest').values.item())\n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean':interp_data, 'Longitude':lons_new, 'Latitude':lats_new, 'model_name': model, 'species': species, 'date': month})\n",
    "            interp_EPA_df = interp_EPA_df.append(tmp_df, sort=False, ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(interp_EPA_df)):\n",
    "    interp_EPA_df.loc[i,('date')] = datetime.datetime(2016,interp_EPA_df['date'][i],calendar.monthrange(2016,interp_EPA_df['date'][i])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPROVE Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolate data for EPA\n",
    "interp_IMPROVE_df = pd.DataFrame(columns=['Arithmetic Mean', 'Longitude', 'Latitude','model_name','species','date'])\n",
    "\n",
    "for model in ['egrid_NA', 'nei_NA', 'epa_NA', 'normal_NA']:\n",
    "    print(model, end = ', ')\n",
    "    for species in ['PM25','NIT','SO4']:\n",
    "        print(species, end = ', ')\n",
    "        for month in np.arange(1,13):\n",
    "            print(month, end = ', ')\n",
    "            #data selected for date\n",
    "            data = poll_ds.sel(model_name = model)[f'{species}'].groupby('time.month').mean().sel(month = month)\n",
    "            \n",
    "            #new lat and lon in radians\n",
    "            if species == 'NH3': #interpolating NH3 to get it for our ISORROPIA total NH4 and NH3\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NH4')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NH4')]['Longitude'].unique()\n",
    "            if species == 'HNO3': #interpolating NH3 to get it for our ISORROPIA total HNO3 and NIT\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NIT')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NIT')]['Longitude'].unique()\n",
    "            if species == 'TotalOC':\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'OC')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'OC')]['Longitude'].unique()\n",
    "            else:\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == species)]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == species)]['Longitude'].unique()\n",
    "            #interpolation function\n",
    "            interp_data = []\n",
    "            for idx in range(lats_new.size):\n",
    "                interp_data.append(data.sel(lat=lats_new[idx], lon=lons_new[idx], method='nearest').values.item())\n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean':interp_data, 'Longitude':lons_new, 'Latitude':lats_new, 'model_name': model, 'species': species, 'date': month})\n",
    "            interp_IMPROVE_df = interp_IMPROVE_df.append(tmp_df, sort=False, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(interp_IMPROVE_df)):\n",
    "    interp_IMPROVE_df.loc[i,('date')] = datetime.datetime(2016,interp_IMPROVE_df['date'][i],calendar.monthrange(2016,interp_IMPROVE_df['date'][i])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a monthly observational dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPROVE_df = IMPROVE_df.rename(columns = {'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monthly_obs_df(obs_df):\n",
    "    \n",
    "    #create the 'geometries' for each lat and lon\n",
    "    gdf = geopandas.GeoDataFrame(\n",
    "    obs_df, geometry=geopandas.points_from_xy(obs_df.Longitude, obs_df.Latitude))\n",
    "    geometries = gdf['geometry'].apply(lambda x: x.wkt).values\n",
    "    #add to the dataset\n",
    "    obs_df['geometry'] = geometries\n",
    "    obs_df.index = obs_df['date']\n",
    "\n",
    "    #group by month and geometry\n",
    "    monthly_obs_df = pd.DataFrame(columns = ['Arithmetic Mean','Latitude','Longitude', 'geometry','species', 'date'])\n",
    "    geometry = geometries[0]\n",
    "    for geometry in np.unique(np.unique(obs_df['geometry'])):\n",
    "        for species in np.unique(obs_df['species'].values):\n",
    "            lat = obs_df.loc[(obs_df['geometry'] == geometry) & (obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Latitude'].first().values\n",
    "            lon = obs_df.loc[(obs_df['geometry'] == geometry) & (obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Longitude'].first().values\n",
    "            data = obs_df.loc[(obs_df['geometry'] == geometry) & (obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Arithmetic Mean'].mean()\n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean': data.values, 'Latitude':lat, 'Longitude':lon, \n",
    "                                   'geometry':geometry, 'species': species, 'date': data.index})\n",
    "            monthly_obs_df = monthly_obs_df.append(tmp_df, sort=False, ignore_index=True)\n",
    "    return(monthly_obs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_IMPROVE_df = create_monthly_obs_df(IMPROVE_df)\n",
    "monthly_EPA_df = create_monthly_obs_df(EPA_obs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Region Names to the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add region to the dataframes based on lat_lon dictionary\n",
    "for df in [monthly_EPA_df, interp_EPA_df, EPA_obs_df, IMPROVE_df, interp_IMPROVE_df,monthly_IMPROVE_df]: \n",
    "    df['Region'] = 'a'\n",
    "    for region in ['SE_lat_lon', 'NW_lat_lon', 'NE_lat_lon', 'MW_lat_lon', 'SW_lat_lon']:\n",
    "        df.loc[(df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & \n",
    "            (df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_EPA_df.to_csv('./final_data/interp_EPA_df.csv', date_format='%Y%m%d', index=False)\n",
    "EPA_obs_df.to_csv('./final_data/EPA_obs_df.csv', date_format='%Y%m%d', index=False)\n",
    "monthly_EPA_df.to_csv('./final_data/EPA_monthly_obs_df.csv', date_format='%Y%m%d', index=False)\n",
    "IMPROVE_df.to_csv('./final_data/IMPROVE_df.csv', date_format='%Y%m%d', index=False)\n",
    "interp_IMPROVE_df.to_csv('./final_data/interp_IMPROVE_df.csv', date_format='%Y%m%d', index=False)\n",
    "monthly_IMPROVE_df.to_csv('./final_data/IMPROVE_monthly_obs_df.csv', date_format='%Y%m%d', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make O3 Dataset Hourly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for poll in ['merged_O3']:\n",
    "    for path in all_paths:\n",
    "        ds6 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_06_hourly.nc')\n",
    "        ds7 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_07_hourly.nc')\n",
    "        ds8 = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_08_hourly.nc')\n",
    "        for idx, ds in zip(['06','07','08'],[ds6, ds7, ds8]):\n",
    "            ds = ds.groupby('time.date').mean(dim = 'time')\n",
    "            ds['date'] = pd.to_datetime(ds['date'])\n",
    "            ds.to_netcdf(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_{idx}_daily_mean.nc4')\n",
    "            print(poll, path, idx, 'done')\n",
    "        ds_JJA = xr.merge([ds6, ds7, ds8])\n",
    "        #save hourly mean DS\n",
    "        ds_JJA.to_netcdf(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/{poll}_JJA_hourly.nc4')\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Health Calculations\n",
    "Using raster = 4 to get the 2015 GWP population density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import our pollution datasets ####\n",
    "ds_PM = xr.open_dataset(\"./final_data/ds_PM_O3_daily.nc4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_import = {}\n",
    "for path in all_paths:\n",
    "    ds_import[path] = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/merged_O3_JJA_hourly.nc4')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsO3_hrly = xr.concat([ds_import[path] for path in all_paths], \n",
    "                      pd.Index(all_paths, name='model_name'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Population Density and Land Area Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import our dataset for population density ####\n",
    "ds = xr.open_dataset(\"/net/fs11/d0/emfreese/population_data/gpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_totpop_2pt5_min_nc/gpw_v4_population_density_adjusted_rev11_2pt5_min.nc\")\n",
    "pop_ds = ds.sel(raster = 4) #density in persons / sq km\n",
    "pop_ds = pop_ds.rename({'latitude':'lat', 'longitude':'lon'})\n",
    "\n",
    "pop_ds = pop_ds.assign_coords(coords = {'lat':pop_ds['lat']}) #fix the lat\n",
    "pop_ds = pop_ds.fillna(0) \n",
    "pop_ds = pop_ds.rename({'UN WPP-Adjusted Population Density, v4.11 (2000, 2005, 2010, 2015, 2020): 2.5 arc-minutes':'pop_density'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pop_ds['pop_density'].plot.imshow(vmax = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### import our dataset for land area ####\n",
    "ds = xr.open_dataset(\"/net/fs11/d0/emfreese/population_data/gpw-v4-population-density-adjusted-to-2015-unwpp-country-totals-rev11_totpop_2pt5_min_nc/gpw_v4_population_density_adjusted_rev11_2pt5_min.nc\")\n",
    "land_ds = ds.sel(raster = 9) #density in persons / sq km\n",
    "land_ds = land_ds.rename({'latitude':'lat', 'longitude':'lon'})\n",
    "\n",
    "land_ds = land_ds.assign_coords(coords = {'lat':land_ds['lat']}) #fix the lat\n",
    "land_ds = land_ds.fillna(0) \n",
    "land_ds = land_ds.rename({'UN WPP-Adjusted Population Density, v4.11 (2000, 2005, 2010, 2015, 2020): 2.5 arc-minutes':'land_area'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "land_ds['land_area'].plot.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Land and Population Datasets, Calculate Total Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merge our pop and land area datasets ####\n",
    "land_pop_ds = xr.merge([pop_ds, land_ds], compat='override')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create lat and lon bounds ####\n",
    "lonb = land_pop_ds['lon'].values + (land_pop_ds['lon'][0]-land_pop_ds['lon'][1]).values/2\n",
    "lonb = np.append(lonb, (lonb[-1] - (land_pop_ds['lon'][0]-land_pop_ds['lon'][1]).values))\n",
    "\n",
    "latb = land_pop_ds['lat'].values + (land_pop_ds['lat'][0]-land_pop_ds['lat'][1]).values/2\n",
    "latb = np.append(latb, (latb[-1] - (land_pop_ds['lat'][0]-land_pop_ds['lat'][1]).values))\n",
    "\n",
    "land_pop_ds['lon_b'] = lonb\n",
    "land_pop_ds['lat_b'] = latb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### find the area of our original grid cells ####\n",
    "A = utils.find_area(land_pop_ds)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### merge to create a new ds with all of our original land and population data ####\n",
    "orig_ds = xr.merge([land_pop_ds, A.to_dataset(name = 'orig_total_area')])\n",
    "\n",
    "orig_ds['land_area_ratio'] = orig_ds['land_area']/orig_ds['orig_total_area']\n",
    "orig_ds['orig_pop_count'] = orig_ds['pop_density']* orig_ds['land_area']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regrid our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a ds with grid for output and grid for input \n",
    "lat_dif = .5 \n",
    "lon_dif = .625\n",
    "ds_out = utils.make_2d_grid(-180, 180, lon_dif, 90, -90, -lat_dif) #grid that we want to regrid to\n",
    "ds_in = utils.make_2d_grid(-180, 180, .041666667, -90, 90, .041666667) #same as the grid for orig_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create regridder (reusing weights)\n",
    "regridder = xe.Regridder(ds_in, ds_out, 'conservative', reuse_weights = True, weights = 'conservative_health_area.nc')\n",
    "regridder  # print basic regridder information.\n",
    "\n",
    "#regrid according to our ds_out grid\n",
    "regrid_ds = regridder(orig_ds)\n",
    "\n",
    "#update the latb and lonb so that they match the ds_out values\n",
    "regrid_ds['lon_b'] = ds_out['lon_b']\n",
    "regrid_ds['lat_b'] = ds_out['lat_b']\n",
    "\n",
    "regrid_ds #look at our regridded dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "regridder.to_netcdf('conservative_health_area.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### find the area of our original grid cells ####\n",
    "A = utils.find_area(regrid_ds)\n",
    "\n",
    "Area_ds = A.to_dataset(name = 'regrid_total_area')\n",
    "Area_ds = Area_ds*-1\n",
    "\n",
    "regrid_area_ds = xr.merge([regrid_ds, Area_ds])\n",
    "regrid_area_ds = regrid_area_ds.drop(['orig_pop_count', 'land_area', 'orig_total_area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create new variables in regridded ds ####\n",
    "\n",
    "regrid_area_ds['regrid_land_area'] = regrid_area_ds['land_area_ratio'] * regrid_area_ds['regrid_total_area'] \n",
    "\n",
    "regrid_area_ds['regrid_pop_count'] = regrid_area_ds['regrid_land_area'] * regrid_area_ds['pop_density']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### check that we still have ~7 billion population ####\n",
    "print(regrid_area_ds['regrid_pop_count'].sum(), orig_ds['orig_pop_count'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mask our data to just the United States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### create a mask ####\n",
    "states = regionmask.defined_regions.natural_earth.us_states_50\n",
    "lon = np.arange(230, 300, .25)\n",
    "lat = np.arange(50, 24, -.125)\n",
    "mask = states.mask(regrid_area_ds, lon_name = 'lon', lat_name = 'lat')\n",
    "contiguous_mask = ~np.isnan(mask) & (mask != 0.) & (mask != 11) \n",
    "\n",
    "#### mask our population dataset ####\n",
    "US_land_pop_ds = regrid_area_ds.where(contiguous_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "US_land_pop_ds['regrid_pop_count'].plot(figsize = (12,5), vmax = 1000000)\n",
    "plt.xlim(-150,-50)\n",
    "plt.ylim(20,50)\n",
    "print('total US population', US_land_pop_ds['regrid_pop_count'].sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_PM = xr.merge([US_land_pop_ds.interp_like(ds_PM), ds_PM])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_PM_orig = ds_PM.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_o3 = xr.merge([US_land_pop_ds.interp_like(ds_PM), dsO3_hrly])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF From Vohra et. al 2021 for PM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beta from Vodonos et al. 2018\n",
    "beta_orig_df = {}\n",
    "for ids in ['low','mean','high']:\n",
    "    beta_orig_df[ids] = pd.read_excel('final_data/CRF_Vodonos_data.xls', names = ['PM25','beta'], sheet_name = ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_row = {}\n",
    "for ids in ['low','mean','high']:\n",
    "    zero_row[ids]= pd.DataFrame({'beta':beta_orig_df[ids]['beta'][0], 'PM25':0}, index = {0})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "β_ds = {}\n",
    "for ids in ['low','mean','high']:\n",
    "    β_ds[ids] = xr.Dataset.from_dataframe(pd.concat([zero_row[ids], beta_orig_df[ids]])).set_index({'index':'PM25'}).rename({'index':'PM25'})\n",
    "    #interpolate beta onto an even grid\n",
    "    β_ds[ids] = β_ds[ids] .interp(PM25 = np.arange(0,70,.0001), method = 'linear')\n",
    "    β_ds[ids]['Δx'] = β_ds[ids]['PM25'].diff(dim = 'PM25')\n",
    "    #convert percent\n",
    "    β_ds[ids]['beta'] /= 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mortality dataΔIHME GBD 2017\n",
    "obs_mortality = pd.read_csv('../../health_data/global_burden_disease_2017_data/IHME-GBD_2017_DATA-2c4a32b7-1.csv')\n",
    "obs_mortality = obs_mortality.drop([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create beta and AF functions\n",
    "def β_mean_(x0, x1, ids):\n",
    "    '''calculates the beta mean for PM2.5 CRF'''\n",
    "    x0 = x0\n",
    "    x1 = x1\n",
    "    if (x0==x1):\n",
    "        return β_ds[ids]['beta'].sel(PM25 = x0, method = 'nearest')\n",
    "    if np.isnan(x0) | np.isnan(x1):\n",
    "        return(np.nan)\n",
    "    if x1>x0:\n",
    "        return((1/(x1-x0))*np.sum((β_ds[ids]['beta']*β_ds[ids]['Δx']).sel(PM25 = slice(x0,x1))))\n",
    "    if x0>x1:\n",
    "        return((1/(x0-x1))*np.sum((β_ds[ids]['beta']*β_ds[ids]['Δx']).sel(PM25 = slice(x1,x0))))\n",
    "\n",
    "    \n",
    "def β_mean(x0, x1,ids):\n",
    "    '''applies the beta mean calculation to our dataset'''\n",
    "    return xr.apply_ufunc(\n",
    "        β_mean_,\n",
    "        x0,\n",
    "        x1,\n",
    "        ids,\n",
    "        input_core_dims=[[],[],[]],  # list with one entry per arg\n",
    "        output_dtypes=[float],\n",
    "        vectorize=True)\n",
    "\n",
    "\n",
    "\n",
    "def AF(ds, x0, x1, ids):\n",
    "    '''calculates the attributable fraction for our CRF for PM2.5'''\n",
    "    return((np.exp(ds[f'β_{ids}']*(x1 - x0))-1)/np.exp(ds[f'β_{ids}']*(x1 - x0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in ['low','mean','high']:\n",
    "    ds_PM[f'β_{ids}'] = xr.zeros_like(ds_PM['PM25']) #create our beta\n",
    "    ds_PM[f'β_{ids}'] = ds_PM[f'β_{ids}'].isel(time = 0).drop('time') #reduce to one time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#drop unnecessary coordinates\n",
    "ds_PM = ds_PM.drop(['lev','raster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our time mean gridded PM values\n",
    "x0 = ds_PM[\"PM25\"].sel(model_name = 'normal_NA').mean(dim = 'time').compute()\n",
    "x1 = ds_PM[\"PM25\"].sel(model_name = 'nonuc_NA').mean(dim = 'time').compute()\n",
    "x2 = ds_PM['PM25'].sel(model_name = 'nonuc_coal_NA').mean(dim = 'time').compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculate the beta for 5, 95 and mean and both the nonuc and nonuc/coal case\n",
    "for ids in ['low','mean','high']:\n",
    "    ds_PM[f'β_{ids}'].loc[dict(model_name = 'nonuc_NA')] = β_mean(x0, x1, ids)#calculate the beta for each grid box\n",
    "    ds_PM[f'β_{ids}'].loc[dict(model_name = 'nonuc_coal_NA')]= β_mean(x0, x2, ids) #calculate the beta for each grid box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in ['low','mean','high']:\n",
    "    ds_PM[f'AF_{ids}'] = xr.zeros_like(ds_PM['PM25']) #create our AF\n",
    "    ds_PM[f'AF_{ids}'] = ds_PM[f'AF_{ids}'].isel(time = 0).drop('time') #reduce to one time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the AF\n",
    "for ids in ['low','mean','high']:\n",
    "    ds_PM[f'AF_{ids}'].loc[dict(model_name = 'nonuc_NA')] =  AF(ds_PM.sel(model_name = 'nonuc_NA'), x0, x1, ids) #calculate the attributable fraction for each grid box\n",
    "    ds_PM[f'AF_{ids}'].loc[dict(model_name = 'nonuc_coal_NA')]=  AF(ds_PM.sel(model_name = 'nonuc_coal_NA'), x0, x2, ids) #calculate the attributable fraction for each grid box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the change in mortality\n",
    "mortality0 = obs_mortality['val']/ds_PM['regrid_pop_count'].sum().values #calculate our initial mortality rate\n",
    "for ids in ['low','mean','high']:\n",
    "    ds_PM[f'Δmortality_{ids}'] = ds_PM[f'AF_{ids}']*ds_PM['regrid_pop_count']*mortality0.values #calculate our change in mortality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_PM['regrid_pop_count'].sum().values #calculate our initial mortality rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF From Turner 2016 for Ozone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(hazard ratio [HR] per 10 ppb, 1.02; 95% confidence interval [CI], 1.01–1.04) from Turner 2016 (which is used in the latest regulatory impact analysis for CSAPR https://www.epa.gov/sites/production/files/2021-03/documents/revised_csapr_update_ria_final.pdf) \n",
    "\n",
    "ΔM≃β_LL×I_OBS×(Δχ×p_aff )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_o3 = ds_o3.isel(lev = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in ['mean','low','high']:\n",
    "    ds_o3[f'Δmortality_{ids}'] = xr.zeros_like(ds_o3['SpeciesConc_O3']) #create our beta\n",
    "    ds_o3[f'Δmortality_{ids}'] = ds_o3[f'Δmortality_{ids}'].isel(time = 0).drop('time') #reduce to one time period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_chi = 10 #ppb\n",
    "RR = {'low':1.01 , 'mean':1.02, 'high': 1.04} #\n",
    "β = {}\n",
    "for ids in ['mean','low','high']:\n",
    "    β[ids]= np.log(RR[ids])/delta_chi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce to local the daytime 8 hour average\n",
    "\n",
    "## https://maps.princeton.edu/catalog/stanford-nt016bt0491\n",
    "tz = geopandas.read_file('final_data/Time_Zones.geojson')\n",
    "#mask based on timezone\n",
    "mask = {}\n",
    "\n",
    "for loc in ['Central','Mountain','Pacific','Eastern']:\n",
    "    mask[loc] = regionmask.Regions(tz.loc[tz['Zone'] == loc]['geometry']).mask(ds_o3, lon_name = 'lon', lat_name = 'lat')\n",
    "\n",
    "    \n",
    "dsO3_masked = {}\n",
    "    \n",
    "for loc in ['Central','Mountain','Pacific','Eastern']:\n",
    "    dsO3_masked[loc] = ds_o3.where(~np.isnan(mask[loc]))\n",
    "\n",
    "#create the change in time zone\n",
    "z_time = np.arange(10,18)\n",
    "z_time_convert = {'Central':5,'Mountain':6,'Pacific':7,'Eastern':4}\n",
    "\n",
    "#apply and merge the different timezones\n",
    "for loc in ['Central','Mountain','Pacific','Eastern']:\n",
    "    dsO3_masked[loc] = dsO3_masked[loc].sel(time = dsO3_hrly.time.dt.hour.isin([z_time-z_time_convert[loc]])).mean(dim = 'time')\n",
    "ds_o3 = xr.merge([dsO3_masked['Central'],dsO3_masked['Mountain'],dsO3_masked['Pacific'],dsO3_masked['Eastern']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = ds_o3['SpeciesConc_O3'].sel(model_name = 'normal_NA').compute()\n",
    "x1 = ds_o3['SpeciesConc_O3'].sel(model_name = 'nonuc_NA').compute()\n",
    "x2 = ds_o3['SpeciesConc_O3'].sel(model_name = 'nonuc_coal_NA').compute()\n",
    "\n",
    "mortality0 = obs_mortality['val']/ds_o3['regrid_pop_count'].sum().values #calculate our initial mortality rate\n",
    "mol_to_ppb = 1e9\n",
    "for ids in ['mean','low','high']:\n",
    "    ds_o3[f'Δmortality_{ids}'].loc[dict(model_name = 'nonuc_NA')] = mol_to_ppb * (x1-x0)*ds_o3['regrid_pop_count']*β[ids]*mortality0.values #calculate our change in mortality\n",
    "    ds_o3[f'Δmortality_{ids}'].loc[dict(model_name = 'nonuc_coal_NA')] = mol_to_ppb * (x2-x0)*ds_o3['regrid_pop_count']*β[ids]*mortality0.values #calculate our change in mortality\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ids in ['mean','low','high']:\n",
    "\n",
    "    print('nonuc', ids, ds_o3.sel(model_name = 'nonuc_NA')[f'Δmortality_{ids}'].sum().values)\n",
    "    print('nonuc_coal', ids, ds_o3.sel(model_name = 'nonuc_coal_NA')[f'Δmortality_{ids}'].sum().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_o3['model_name'] = ds_o3.model_name.astype('str')\n",
    "#ds_PM['model_name'] = ds_PM.model_name.astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.Dataset.to_zarr(ds_o3, './final_data/o3_mortalities.zarr', mode = 'w') #save the dataset \n",
    "#xr.Dataset.to_zarr(ds_PM, './final_data/pm_mortalities.zarr', mode = 'w') #save the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate pollution data by county and assign nuclear/coal adjacent or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_o3 = xr.open_zarr('./final_data/o3_mortalities.zarr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths = ['nei_NA','normal_NA','nonuc_NA','nonuc_coal_NA','egrid_NA','epa_NA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import hourly summertime ozone data\n",
    "ds_import = {}\n",
    "for path in all_paths:\n",
    "    ds_import[path] = xr.open_dataset(f'/net/fs11/d0/emfreese/GCrundirs/nuclearproj/{path}/merged_data/merged_O3_JJA_hourly.nc4')\n",
    "    \n",
    "dsO3_hrly = xr.concat([ds_import[path] for path in all_paths], \n",
    "                      pd.Index(all_paths, name='model_name'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import daily pm data\n",
    "poll_ds = xr.open_dataset('final_data/ds_PM_O3_daily.nc4')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce to just the daytime 8 hour average\n",
    "\n",
    "## https://maps.princeton.edu/catalog/stanford-nt016bt0491\n",
    "tz = geopandas.read_file('final_data/Time_Zones.geojson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = {}\n",
    "\n",
    "for loc in ['Central','Mountain','Pacific','Eastern']:\n",
    "    mask[loc] = regionmask.Regions(tz.loc[tz['Zone'] == loc]['geometry']).mask(dsO3_hrly, lon_name = 'lon', lat_name = 'lat')\n",
    "    \n",
    "    dsO3_masked = {}\n",
    "for loc in ['Central','Mountain','Pacific','Eastern']:\n",
    "    dsO3_masked[loc] = dsO3_hrly.where(~np.isnan(mask[loc]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_time = np.arange(10,18)\n",
    "z_time_convert = {'Central':5,'Mountain':6,'Pacific':7,'Eastern':4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loc in ['Central','Mountain','Pacific','Eastern']:\n",
    "    dsO3_masked[loc] = dsO3_masked[loc].sel(time = dsO3_hrly.time.dt.hour.isin([z_time-z_time_convert[loc]])).isel(lev = 0).mean(dim = 'time')\n",
    "ds_hrly = xr.merge([dsO3_masked['Central'],dsO3_masked['Mountain'],dsO3_masked['Pacific'],dsO3_masked['Eastern']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import county and state data\n",
    "#https://www.census.gov/geographies/mapping-files/time-series/geo/carto-boundary-file.html\n",
    "\n",
    "counties = geopandas.read_file('final_data/cb_2016_us_county_5m.shp')\n",
    "counties = counties.rename(columns = {'NAME':'CountyName'})\n",
    "\n",
    "states = geopandas.read_file('data/cb_2018_us_state_500k.shx')\n",
    "states = states.rename(columns = {'NAME':'StateName'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.loc[(counties['CountyName'] == 'Richmond')&(counties['COUNTYFP'] == '159'), 'CountyName'] = 'Richmond City' #city\n",
    "counties.loc[(counties['CountyName'] == 'Richmond')&(counties['COUNTYFP'] == '760'), 'CountyName'] = 'Richmond County'\n",
    "\n",
    "\n",
    "counties.loc[(counties['CountyName'] == 'Baltimore')&(counties['COUNTYFP'] == '510'), 'CountyName'] = 'Baltimore City'\n",
    "counties.loc[(counties['CountyName'] == 'Baltimore')&(counties['COUNTYFP'] == '005'), 'CountyName'] = 'Baltimore County'\n",
    "\n",
    "counties.loc[(counties['CountyName'] == 'St. Louis')&(counties['COUNTYFP'] == '510'), 'CountyName'] = 'St. Louis City'\n",
    "counties.loc[(counties['CountyName'] == 'St. Louis')&(counties['COUNTYFP'] == '189'), 'CountyName'] = 'St. Louis County'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ESMPyManager:\n",
       "    local_pet = 0\n",
       "    pet_count = 1\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ESMF\n",
    "ESMF.Manager(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emfreese/anaconda3/envs/grid_mod/lib/python3.9/site-packages/xesmf/frontend.py:915: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  weights = weights.multiply(1 / weights.sum(axis=0))\n"
     ]
    }
   ],
   "source": [
    "savg = xe.SpatialAverager(poll_ds.mean(dim = 'time'), counties.geometry, geom_dim_name=\"county\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emfreese/anaconda3/envs/grid_mod/lib/python3.9/site-packages/xesmf/frontend.py:466: FutureWarning: ``output_sizes`` should be given in the ``dask_gufunc_kwargs`` parameter. It will be removed as direct parameter in a future version.\n",
      "  dr_out = xr.apply_ufunc(\n"
     ]
    }
   ],
   "source": [
    "out = savg(poll_ds.PM25)\n",
    "out = out.assign_coords(county=xr.DataArray(counties[\"CountyName\"], dims=(\"county\",)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "out2 = savg(ds_hrly.SpeciesConc_O3)\n",
    "out2 = out2.assign_coords(county=xr.DataArray(counties[\"CountyName\"], dims=(\"county\",)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out3 = savg(ds_o3.regrid_pop_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "out3 = out3.assign_coords(county=xr.DataArray(counties[\"CountyName\"], dims=(\"county\",)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "states_counties['pop_count'] = out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['PM25_nonuc'] = (out.sel(model_name = 'nonuc_NA').mean(dim = 'time') - \n",
    "                          out.sel(model_name = 'normal_NA').mean(dim = 'time')).values\n",
    "counties['PM25_nonuc_coal'] = (out.sel(model_name = 'nonuc_coal_NA').mean(dim = 'time') - \n",
    "                               out.sel(model_name = 'normal_NA').mean(dim = 'time')).values\n",
    "\n",
    "counties['O3_nonuc'] = (out2.sel(model_name = 'nonuc_NA') - \n",
    "                        out2.sel(model_name = 'normal_NA')).values*1e9 #convert to ppb\n",
    "counties['O3_nonuc_coal'] = (out2.sel(model_name = 'nonuc_coal_NA') - \n",
    "                             out2.sel(model_name = 'normal_NA')).values*1e9 #convert to ppb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modify the county and state data types\n",
    "counties = counties.reset_index()\n",
    "counties['index'] = counties['index'].astype('int32')\n",
    "counties['STATEFP'] = counties['STATEFP'].astype('int32')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our locations of coal plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = feather.read_dataframe(f'../optimization_model/outputs/gen_normal.feather')\n",
    "carac = pd.read_csv(f'../optimization_model/good_model_inputs/inputs_gen_normal.csv')\n",
    "carac.loc[carac['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "# Clean columns name\n",
    "carac = carac.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Merge\n",
    "df_normal = pd.concat((carac,gen), axis=1)\n",
    "df_normal['2016_365_23'] = df_normal['2016_365_22'].copy()\n",
    "\n",
    "col_list = list(gen.columns)\n",
    "col_list.append('2016_365_23')\n",
    "df_normal['total_gen'] = df_normal[col_list].sum(axis=1)\n",
    "df_normal.loc[df_normal['CountyName'] == 'La Salle','CountyName'] = 'LaSalle'\n",
    "df_normal.loc[df_normal['CountyName'] == 'WestChester','CountyName'] = 'Westchester'\n",
    "df_normal.loc[df_normal['CountyName'] == 'Apache', 'StateName'] = 'Arizona'\n",
    "df_normal.loc[df_normal['CountyName'] == 'Dona Ana','CountyName'] = 'Doña Ana'\n",
    "df_normal.loc[df_normal['CountyName'] == 'St Charles','CountyName'] = 'St. Charles'\n",
    "df_normal.loc[df_normal['CountyName'] == 'St Clair','CountyName'] = 'St. Clair'\n",
    "\n",
    "df_normal['CountyName'] = df_normal['CountyName'].str.lower()\n",
    "df_normal['StateName'] = df_normal['StateName'].str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coal = df_normal[df_normal['FuelType'] == 'Coal'][['LAT','LON']].groupby(['LAT','LON']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_coal = geopandas.GeoDataFrame(\n",
    "    df_coal, geometry=geopandas.points_from_xy(df_coal.LON, df_coal.LAT))\n",
    "gdf_coal['geometry'].crs = counties['geometry'].crs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_coal = gdf_coal.to_crs(counties.geometry.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our locations of the nuclear plants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nuclear = df_normal[df_normal['FuelType'] == 'Nuclear'][['LAT','LON']].groupby(['LAT','LON']).mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_nuclear = geopandas.GeoDataFrame(\n",
    "    df_nuclear, geometry=geopandas.points_from_xy(df_nuclear.LON, df_nuclear.LAT))\n",
    "gdf_nuclear['geometry'].crs = counties['geometry'].crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate those counties with a distance < 50 miles from a power plant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties['nuclear_adjacent'] = False\n",
    "counties['coal_containing'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties.loc[counties['COUNTYNS'].isin(geopandas.sjoin(left_df=gdf_coal, right_df=counties, how='inner')['COUNTYNS']), 'coal_containing'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for county in counties['geometry']:\n",
    "    for nuc in gdf_nuclear['geometry']:\n",
    "        poly = county\n",
    "        point = nuc\n",
    "        # The points are returned in the same order as the input geometries:\n",
    "        p1, p2 = nearest_points(poly, point)\n",
    "        #print(utils.haversine(p1.x, p1.y, p2.x, p2.y))\n",
    "        if utils.haversine(p1.x, p1.y, p2.x, p2.y) <= 80.4672:\n",
    "            counties.loc[counties['geometry'] == county,'nuclear_adjacent'] = True\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include State and County Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['STATEFP'] = states['STATEFP'].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_counties = states.merge(counties, on = 'STATEFP', suffixes=('_state', '_county'))[['index','CountyName','StateName','nuclear_adjacent','coal_containing','COUNTYNS','geometry_county',\n",
    "                                                                                         'PM25_nonuc', 'PM25_nonuc_coal', 'O3_nonuc', 'O3_nonuc_coal']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_counties['County_State'] = states_counties[['CountyName','StateName']].apply(lambda x: ', '.join(x[x.notnull()]), axis = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_counties.to_csv('./final_data/df_pm_o3_county_pop.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISORROPIA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GC run data\n",
    "poll_ds = xr.open_zarr('./data/GC_output.zarr')\n",
    "\n",
    "#import RH and T dataframe\n",
    "RH_T_df = pd.read_csv('./data/RH_T.csv', index_col=[0,1])\n",
    "\n",
    "#subset our ds into just the species we need\n",
    "species_list = ['SO4','NH3','HNO3','NIT','NH4']\n",
    "isorropia_ds = poll_ds[species_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### species in ISORROPIA \n",
    "SO4, NH3, NO3, Cl, Ca, K, Mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for our isorropia monthly mean values for no nuclear and normal cases, indexed by species, location, season\n",
    "\n",
    "isorropia_dict = {}\n",
    "for species in species_list:\n",
    "    isorropia_dict[species]= {}\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "        isorropia_dict[species][region] = {}\n",
    "        for season in ['DJF','MAM','JJA','SON']:\n",
    "            data = isorropia_ds.sel(lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "                                    lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])).groupby('time.season').mean(dim = ['lat','lon','time']).sel(season = season)[species]\n",
    "            isorropia_dict[species][region][season] = data.values\n",
    "            \n",
    "isorropia_df = pd.DataFrame.from_dict({(i,j,k): isorropia_dict[i][j][k] \n",
    "                            for i in isorropia_dict.keys() \n",
    "                            for j in isorropia_dict[i].keys()\n",
    "                            for k in isorropia_dict[i][j].keys()},\n",
    "                            orient='index', columns = isorropia_ds['model_name'].values)\n",
    "\n",
    "isorropia_df.index = pd.MultiIndex.from_tuples(isorropia_df.index, names = ['Species','Location', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from mol/mol to mol/m3 by multiplying by P/T*R (mol/m3)\n",
    "stp_p = 101325 #Pa = kg/m/s^2\n",
    "R = 8.314 #J/K/mol\n",
    "for species in species_list:\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "            for season in ['DJF','MAM','JJA','SON']:\n",
    "                isorropia_df.loc[(species,region,season)] *= stp_p/R/RH_T_df.loc[(region,season)]['T']\n",
    "                #isorropia_df.loc[(species,region,season)]['normal'] *= stp_p/R/RH_T_df.loc[(region,season)]['T']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert NIT, NH4, SO4 from ug/m3 to mol/m3\n",
    "for species in utils.aerosol_species_dict.keys():\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "            for season in ['DJF','MAM','JJA','SON']:\n",
    "                isorropia_df.loc[(species,region,season)] /= (utils.aerosol_species_dict[species]*1e6)#ug/m3/(#g/mol*#ug/g)\n",
    "\n",
    "#convert NH3, HNO3 from ppbv to mol/m3\n",
    "for species in ['NH3','HNO3']:\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "            for season in ['DJF','MAM','JJA','SON']:\n",
    "                isorropia_df.loc[(species,region,season)] *= stp_p/R/RH_T_df.loc[(region,season)]['T']/1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the total Nitrate and ammonium by molarity\n",
    "isorropia_df_TNO3 = isorropia_df.loc['HNO3'] + isorropia_df.loc['NIT'] \n",
    "isorropia_df_TNH3 = isorropia_df.loc['NH3'] + isorropia_df.loc['NH4'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run through ISORROPIA\n",
    "### Only run once to initiate a new ISORROPIA output, otherwise, the output is already made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/net/fs11/d0/emfreese/ISORROPIAIIStandalone')\n",
    "for model in ['nonuc_model', 'normal_model']:\n",
    "    for region in ['SE_lat_lon', 'NW_lat_lon', 'NE_lat_lon', 'MW_lat_lon', 'SW_lat_lon']:\n",
    "            for season in ['DJF','JJA']:\n",
    "                T_tmp = RH_T_df.loc[(region, season)]['T']\n",
    "                RH_tmp = RH_T_df.loc[(region, season)]['RH']\n",
    "                SO4_tmp = isorropia_df.loc['SO4',region,season][model]\n",
    "                \n",
    "                os.system(f'mkdir -p {region}_{season}_{model}/')\n",
    "                os.system(f'cp src/* {region}_{season}_{model}/')\n",
    "                \n",
    "                os.chdir(f'{region}_{season}_{model}/')\n",
    "                print(os.listdir())\n",
    "                \n",
    "                cmdprefix = 'cat ISORange.dat | sed -i '\n",
    "                cmdT = f\"-e 's/T(K):       0.0/T(K):       {T_tmp}/' \"\n",
    "                cmdRH = f\"-e 's/RH(-):      0.0/RH(-):      {RH_tmp}/' \"\n",
    "                cmdNH3 = f\"-e 's/TSO4:       0.0e-9/TSO4:       {SO4_tmp}/' \"\n",
    "                cmdoutput = f\"-e 's/Output:     NW_DJF_nonuc_ISOOutput.nc/Output:     ISOOutput_{region}_{season}_{model}_constant_SO4.nc/' \"\n",
    "                cmdsuffix = 'ISORange.dat'\n",
    "                \n",
    "                cmd = cmdprefix+cmdT+cmdRH+cmdNH3+cmdoutput+cmdsuffix\n",
    "                \n",
    "                os.system(cmd)\n",
    "                \n",
    "                os.chdir('../')\n",
    "\n",
    "os.chdir('../grid_model/ego_nonuclear_project/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save our data out ####\n",
    "isorropia_df.to_csv('./data/ISORROPIA_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python grid_mod",
   "language": "python",
   "name": "grid_mod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
