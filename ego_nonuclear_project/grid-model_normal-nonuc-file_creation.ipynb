{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import regionmask\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import cartopy.feature as cfeat\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "import feather\n",
    "\n",
    "import glob\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import plotting\n",
    "\n",
    "np.seterr(invalid='ignore'); # disable a warning from matplotlib and cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concentration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normal_model = utils.import_GC_runs_general('../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation/OutputDir/','GEOSChem.SpeciesConc.2016*_0000z.nc4','GEOSChem.AerosolMass.2016*_0000z.nc4','normal_model' )\n",
    "ds_nonuc_model = utils.import_GC_runs_general('../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_nonuclear/OutputDir/','GEOSChem.SpeciesConc.2016*_0000z.nc4','GEOSChem.AerosolMass.2016*_0000z.nc4','nonuc_model' )\n",
    "\n",
    "datasets = [\n",
    "    ds_normal_model,\n",
    "    ds_nonuc_model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_ds = utils.combine_and_convert_ds(utils.gas_species_list, utils.aerosol_species_dict.keys(), datasets,['normal_model', 'nonuc_model'], ['nonuc_model'], 'normal_model', 'normal and nonuclear' )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f26ca3eb620>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poll_ds.to_zarr('nonuc_normal_GC.zarr', mode = 'w') #save the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEMCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normal = xr.open_mfdataset('../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation/OutputDir/HEMCO_diagnostics.2016*.nc', combine = 'by_coords')\n",
    "ds_nonuc = xr.open_mfdataset('../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_nonuclear/OutputDir/HEMCO_diagnostics.2016*.nc', combine = 'by_coords')\n",
    "\n",
    "ds_normal = ds_normal.isel(lev = 0)\n",
    "ds_nonuc = ds_nonuc.isel(lev = 0)\n",
    "\n",
    "ds_emis = xr.concat([ds_nonuc, ds_normal], pd.Index(['nonuc', 'normal'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_list = ['EmisNO2_Anthro', 'EmisNO_Anthro', 'EmisSO2_Anthro',  'EmisNH3_Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_ds = ds_emis.drop_vars([species for species in ds_emis.data_vars if species not in emis_list])\n",
    "emis_ds['EmisNOx_Anthro'] = (\n",
    "        emis_ds['EmisNO2_Anthro'] + emis_ds['EmisNO_Anthro'] #sum our NO2 and NO to get NOx\n",
    "                   )\n",
    "emis_ds['EmisNOx_Anthro'].attrs['units'] = 'kg/m2/s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f647a315f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emis_ds.to_zarr('emissions_HEMCO', mode = 'w') #save the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-EGO Emissions Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import files and change hydro #####\n",
    "\n",
    "###get ORIS, plant type, and Region Name from our modified generation file\n",
    "oris_nonuc_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_no-nuclear_all-generators_20k-new_name.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "oris_normal_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_normal.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "oris_nonuc_df.loc[oris_nonuc_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "oris_normal_df.loc[oris_normal_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "\n",
    "###import egrid generation data and modify the index to be ORISCode\n",
    "egrid_df_raw = pd.read_excel('../raw_data/egrid2016_data.xlsx',sheet_name='GEN16', usecols='D, J, L')\n",
    "egrid = egrid_df_raw.drop(egrid_df_raw.index[0]).rename(columns={'DOE/EIA ORIS plant or facility code':'ORISCode'})\n",
    "#change those with no capacity to 1 (so that our capacity factor calculation doesn't fail)\n",
    "egrid.loc[egrid['Generator nameplate capacity (MW)'] == 0, 'Generator nameplate capacity (MW)'] = 1\n",
    "#calculate the capacity factor of generation in egrid\n",
    "egrid['egrid_capafactor'] = egrid['Generator annual net generation (MWh)'] / (8760 * egrid['Generator nameplate capacity (MW)']) \n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_normal_df=feather.read_dataframe('../optimization_model/outputs/gen_normal.feather')\n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_nonuc_df=feather.read_dataframe('../optimization_model/outputs/gen_no-nuclear_modified-all-generators.feather')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### change index and convert to xarray #####\n",
    "\n",
    "#group by ORIS code and make a date and ORIS code mult index\n",
    "gen_nonuc_df = pd.concat([gen_nonuc_df,oris_nonuc_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "gen_normal_df = pd.concat([gen_normal_df,oris_normal_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "gen_normal_df = gen_normal_df.stack()\n",
    "gen_nonuc_df = gen_nonuc_df.stack()\n",
    "gen_nonuc_df.index.names = (['ORISCode','date'])\n",
    "gen_normal_df.index.names = (['ORISCode','date'])\n",
    "\n",
    "#convert to xarray datasets\n",
    "oris_nonuc_ds = oris_nonuc_df.to_xarray()\n",
    "oris_normal_ds = oris_normal_df.to_xarray()\n",
    "egrid_ds = egrid.to_xarray()\n",
    "gen_normal_ds = gen_normal_df.to_xarray()\n",
    "gen_nonuc_ds = gen_nonuc_df.to_xarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One or more of the specified variables cannot be found in this dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-26a5d5c4f80a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m### group egrid by ORIS code, sum, and drop the capacity factor (because it wouldn't be the sum)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0megrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0megrid_ds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ORISCode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'egrid_capafactor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, dim, errors, **labels_kwargs)\u001b[0m\n\u001b[1;32m   3686\u001b[0m                 \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3687\u001b[0m             )\n\u001b[0;32m-> 3688\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_vars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3689\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3690\u001b[0m             warnings.warn(\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36mdrop_vars\u001b[0;34m(self, names, errors)\u001b[0m\n\u001b[1;32m   3645\u001b[0m             \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3646\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3647\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_all_in_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3649\u001b[0m         \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variables\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/conda_env/lib/python3.7/site-packages/xarray/core/dataset.py\u001b[0m in \u001b[0;36m_assert_all_in_dataset\u001b[0;34m(self, names, virtual_okay)\u001b[0m\n\u001b[1;32m   3616\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbad_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3617\u001b[0m             raise ValueError(\n\u001b[0;32m-> 3618\u001b[0;31m                 \u001b[0;34m\"One or more of the specified variables \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3619\u001b[0m                 \u001b[0;34m\"cannot be found in this dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3620\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: One or more of the specified variables cannot be found in this dataset"
     ]
    }
   ],
   "source": [
    "###### setup for our datasets #######\n",
    "\n",
    "###create dicts of our datasets/names\n",
    "gen_names_dict = {'normal':gen_normal_ds,'nonuc':gen_nonuc_ds}\n",
    "oris_names_dict = {'normal':oris_normal_ds,'nonuc':oris_nonuc_ds}\n",
    "models = ['normal','nonuc']\n",
    "\n",
    "### group egrid by ORIS code, sum, and drop the capacity factor (because it wouldn't be the sum)\n",
    "egrid = egrid_ds.groupby('ORISCode').sum().drop('egrid_capafactor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### concatenate our different dataset components together #######\n",
    "\n",
    "### define dicts for output\n",
    "capacity = {}\n",
    "fueltype = {}\n",
    "regionname = {}\n",
    "gmodel_oris = {}\n",
    "gmodel_egrid = {}\n",
    "\n",
    "for name in models:\n",
    "    ###set up inputs\n",
    "    #create a capacity, fueltype, and regionname dict grouped by ORISCode\n",
    "    capacity[name] = oris_names_dict[name].groupby('ORISCode').sum()['Capacity']\n",
    "    fueltype[name]= oris_names_dict[name].to_dataframe().groupby('ORISCode').first()['FuelType']\n",
    "    regionname[name]= oris_names_dict[name].to_dataframe().groupby('ORISCode').first()['RegionName']\n",
    "    #group ORIS dicts by ORIS code and take the mean of everything but capacity\n",
    "    oris_names_dict[name] = oris_names_dict[name].groupby('ORISCode').mean().drop('Capacity')\n",
    "    #rename our generation variable\n",
    "    gen_names_dict[name] = gen_names_dict[name].rename({'__xarray_dataarray_variable__':'modelgeneration'})\n",
    "\n",
    "    ###concatenate the generation and ORIS files\n",
    "    gmodel_oris[name] = xr.merge([gen_names_dict[name], oris_names_dict[name]])\n",
    "    #add in the capacity\n",
    "    gmodel_oris[name]['Capacity'] = capacity[name]\n",
    "    #create a column for capacity factors\n",
    "    gmodel_oris[name]['model_capafactor'] = 100 * gmodel_oris[name]['modelgeneration'] / (gmodel_oris[name]['Capacity'] * 8760) # % generation for each year's total capacity\n",
    "    \n",
    "    ###concatenate our model/oris and egrid emissions dataframes into one, grouped by ORIS code\n",
    "    gmodel_egrid[name] = xr.merge([gmodel_oris[name],egrid])\n",
    "    #turn all zeroes (just in the modelgeneration) to NAN\n",
    "    gmodel_egrid[name].where('modelgeneration' == 0)['modelgeneration'] = np.nan\n",
    "    #rename the egrid data column for ease\n",
    "    gmodel_egrid[name] = gmodel_egrid[name].rename({'Generator annual net generation (MWh)':'egridgeneration'})\n",
    "    #add in fueltype\n",
    "    gmodel_egrid[name]['fueltype'] = fueltype[name]\n",
    "    gmodel_egrid[name] = gmodel_egrid[name].set_coords('fueltype')\n",
    "    #add in region name\n",
    "    gmodel_egrid[name]['regionname'] = regionname[name]\n",
    "    gmodel_egrid[name] = gmodel_egrid[name].set_coords('regionname')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### fix datetime and add dec 31st at the 23rd hour ######\n",
    "\n",
    "###create a dict for dec 31st 23rd hour (just copy the dec 31 22nd hour data over)\n",
    "ds_dec23 = {}\n",
    "for name in models:\n",
    "    ds_dec23[name] = gmodel_egrid[name].isel(date = [-1])\n",
    "    #change date to datetime for the 23rd hour\n",
    "    ds_dec23[name]['date'] = [datetime.datetime(2017,12,31,23,0)]\n",
    "\n",
    "###change all dates to datetime\n",
    "#make a list of dates for the year\n",
    "base = datetime.datetime(2017, 1, 1) #base date\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(8759)] #loop through all hours but the final one (we will add it in above)\n",
    "date_list\n",
    "for name in models:\n",
    "    gmodel_egrid[name]['date'] = date_list\n",
    "    gmodel_egrid[name] = xr.merge([gmodel_egrid[name],ds_dec23[name]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "To convert our generation in MWh to get the emissions in kg/sec, we use the following (and making sure our emissions rates are in kg/sec)\n",
    "\n",
    "$MWH/3600sec -> (MW/s) * kg/MW -> kg/s$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### add NOx, SO2, CO2, CH4 emissions to gmodel_egrid dataset ######\n",
    "\n",
    "no_mult = 0.8544304 # NO/NOx as estimated from NEI2011 inventory\n",
    "no2_mult = 1 - 0.8544304 # NO2/NOx as estimated from NEI2011 inventory\n",
    "### process our emissions by multiplying generation* emissions factors\n",
    "for name in models:\n",
    "    gmodel_egrid[name]['NO']  = no_mult * gmodel_egrid[name]['modelgeneration']/3600 * gmodel_egrid[name]['PLNOXRTA']\n",
    "    gmodel_egrid[name]['NO2']  = no2_mult * gmodel_egrid[name]['modelgeneration']/3600 * gmodel_egrid[name]['PLNOXRTA']\n",
    "    gmodel_egrid[name]['SO2']  =  gmodel_egrid[name]['modelgeneration']/3600 * gmodel_egrid[name]['PLSO2RTA']\n",
    "    gmodel_egrid[name]['CO2']  =  gmodel_egrid[name]['modelgeneration']/3600 * gmodel_egrid[name]['PLCO2RTA']\n",
    "    gmodel_egrid[name]['CH4']  =  gmodel_egrid[name]['modelgeneration']/3600 * gmodel_egrid[name]['PLCH4RTA']\n",
    "\n",
    "##### annual generation in original dataset ######\n",
    "    gmodel_egrid[name]['annual_modelgeneration'] = gmodel_egrid[name]['modelgeneration'].groupby('ORISCode').sum(dim = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### create new datasets for region and fuel type grouped data, add annual generation to normal ds ######\n",
    "###dicts for datasets\n",
    "fueltype_grouped_ds = {}\n",
    "regionname_grouped_ds = {}\n",
    "\n",
    "###list of pollutants\n",
    "pollutants = ['NO','NO2','SO2','CO2','CH4']\n",
    "\n",
    "for name in models:\n",
    "    \n",
    "###new datasets    \n",
    "    #annual generation by fuel type\n",
    "    fueltype_grouped_ds[name] = gmodel_egrid[name].groupby('fueltype').sum(dim = 'ORISCode')\n",
    "    fueltype_grouped_ds[name].drop(['model_capafactor','PLCH4RTA','PLCO2RTA','PLNOXRTA','PLSO2RTA','PLN2ORTA'])\n",
    "    \n",
    "    #annual generation by region\n",
    "    regionname_grouped_ds[name] = gmodel_egrid[name].groupby('regionname').sum(dim = 'ORISCode')\n",
    "    regionname_grouped_ds[name].drop(['model_capafactor','PLCH4RTA','PLCO2RTA','PLNOXRTA','PLSO2RTA','PLN2ORTA'])\n",
    "\n",
    "    #add annual emissions of each pollutant\n",
    "    for pollutant in pollutants:\n",
    "        regionname_grouped_ds[name][f'annual_{pollutant}'] = regionname_grouped_ds[name][f'{pollutant}'].sum(dim = 'date')\n",
    "        fueltype_grouped_ds[name][f'annual_{pollutant}'] = fueltype_grouped_ds[name][f'{pollutant}'].sum(dim = 'date')\n",
    "#add ratio of difference in emissions normal-nonuc/ difference in generation normal-nonuc to the nonuc dataset\n",
    "for pollutant in pollutants:\n",
    "    fueltype_grouped_ds['nonuc'][f'normal-nonuc_{pollutant}-gen_ratio'] = (fueltype_grouped_ds['normal'][f'annual_{pollutant}']-\n",
    "                                                                           fueltype_grouped_ds['nonuc'][f'annual_{pollutant}'])/(fueltype_grouped_ds['normal']['annual_modelgeneration']-\n",
    "                                                                                                                                 fueltype_grouped_ds['nonuc']['annual_modelgeneration'])\n",
    "    regionname_grouped_ds['nonuc'][f'normal-nonuc_{pollutant}-gen_ratio'] = (regionname_grouped_ds['normal'][f'annual_{pollutant}']-\n",
    "                                                                             regionname_grouped_ds['nonuc'][f'annual_{pollutant}'])/(regionname_grouped_ds['normal']['annual_modelgeneration']-\n",
    "                                                                                                                                     regionname_grouped_ds['nonuc']['annual_modelgeneration'])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save datasets #####\n",
    "\n",
    "### Final datasets to save are: gmodel_egrid, fueltype_grouped_ds, regionname_grouped_ds\n",
    "for name in models:\n",
    "    regionname_grouped_ds[name].to_netcdf(f'./regionname_grouped_emissions_{name}_ds.nc', 'w')\n",
    "    fueltype_grouped_ds[name].to_netcdf(f'./fueltype_grouped_emissions_{name}_ds.nc', 'w')\n",
    "    gmodel_egrid[name].to_netcdf(f'./gmodel_egrid_emissions_{name}_ds.nc', 'w')\n",
    "\n",
    "### datasets partway to save are:\n",
    "\n",
    "oris_nonuc_ds.to_netcdf('../optimization_model/outputs/oris_nonuc_ds.nc', 'w')\n",
    "oris_normal_ds.to_netcdf('../optimization_model/outputs/oris_normal_ds.nc', 'w')\n",
    "egrid_ds.to_netcdf('../optimization_model/outputs/egrid_ds.nc', 'w')\n",
    "gen_normal_ds.to_netcdf('../optimization_model/outputs/gen_normal_ds.nc', 'w')\n",
    "gen_nonuc_ds.to_netcdf('../optimization_model/outputs/gen_nonuc_ds.nc', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERRA2 RH and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import MERRA2 datasets for RH and T #####\n",
    "\n",
    "#import \n",
    "T_ds = xr.open_mfdataset('../../MERRA2/2016/GEOS_0.5x0.625_NA/MERRA2/2016/*/MERRA2.2016*.A1.05x0625.NA.nc4', combine = 'by_coords')\n",
    "RH_ds = xr.open_mfdataset('../../MERRA2/2016/GEOS_0.5x0.625_NA/MERRA2/2016/*/MERRA2.2016*.A3dyn.05x0625.NA.nc4', combine = 'by_coords')\n",
    "\n",
    "#reduce datasets just to T and RH\n",
    "T_ds = T_ds['TS']\n",
    "RH_ds = RH_ds['RH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Select T and RH in the bounds of our US lat and lon #####\n",
    "T = {}\n",
    "RH = {}\n",
    "for region in utils.lat_lon_dict.keys():\n",
    "    T[region] = {}\n",
    "    T[region] = T_ds.sel(\n",
    "        lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "        lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])\n",
    "    ).groupby('time.season').mean()\n",
    "    RH[region] = {}\n",
    "    RH[region] = RH_ds.sel(\n",
    "        lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "        lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])\n",
    "    ).groupby('time.season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate seasonal mean RH and T by region #####\n",
    "T_seasonal_mean = {}\n",
    "RH_seasonal_mean = {}\n",
    "for region in utils.lat_lon_dict.keys():\n",
    "    T_seasonal_mean[region] = {}\n",
    "    RH_seasonal_mean[region] = {}\n",
    "    for seasons in ['DJF','JJA','MAM','SON']:\n",
    "        T_seasonal_mean[region][seasons] = T[region].sel(season = seasons).mean().values\n",
    "        RH_seasonal_mean[region][seasons] = RH[region].sel(season = seasons).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### convert to dataframes ##### \n",
    "\n",
    "T_df = pd.DataFrame.from_dict({(i,j): T_seasonal_mean[i][j]\n",
    "                            for i in T_seasonal_mean.keys() \n",
    "                            for j in T_seasonal_mean[i].keys()},\n",
    "                            orient='index', columns = ['T'])\n",
    "T_df.index = pd.MultiIndex.from_tuples(T_df.index)\n",
    "\n",
    "RH_df = pd.DataFrame.from_dict({(i,j): RH_seasonal_mean[i][j]\n",
    "                            for i in RH_seasonal_mean.keys() \n",
    "                            for j in RH_seasonal_mean[i].keys()},\n",
    "                            orient='index', columns = ['RH'])\n",
    "RH_df.index = pd.MultiIndex.from_tuples(RH_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### merge dataframes #####\n",
    "RH_T_df = pd.merge(\n",
    "    T_df, RH_df, \n",
    "    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save dataframe #####\n",
    "pd.DataFrame.to_csv(RH_T_df, './RH_T.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
