{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import regionmask\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import cartopy.feature as cfeat\n",
    "import matplotlib.patches as mpatches\n",
    "import datetime\n",
    "\n",
    "import geopandas\n",
    "\n",
    "import feather\n",
    "\n",
    "import glob\n",
    "\n",
    "import calendar\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "import plotting\n",
    "\n",
    "np.seterr(invalid='ignore'); # disable a warning from matplotlib and cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GEOS Chem Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "egrid_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation-egrid/OutputDir/'\n",
    "NEIref_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gtropchem/merra2_05x0625_tropchem_na/OutputDir/'\n",
    "normal_model_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation/OutputDir/'\n",
    "nonuc_model_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_nonuclear/OutputDir/'\n",
    "\n",
    "speciesconc_output = 'GEOSChem.SpeciesConc.2016*.nc4'\n",
    "aerosol_output = 'GEOSChem.AerosolMass.2016*.nc4'\n",
    "\n",
    "ds_egrid = utils.import_GC_runs_general(egrid_path, speciesconc_output, aerosol_output, 'egrid')\n",
    "ds_NEIref = utils.import_GC_runs_general(NEIref_path, speciesconc_output, aerosol_output, 'NEIref')\n",
    "ds_normal_model = utils.import_GC_runs_general(normal_model_path, speciesconc_output, aerosol_output, 'normal_model')\n",
    "ds_nonuc_model = utils.import_GC_runs_general(nonuc_model_path, speciesconc_output, aerosol_output, 'nonuc_model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    ds_egrid,\n",
    "    ds_NEIref,\n",
    "    ds_normal_model,\n",
    "    ds_nonuc_model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_ds = utils.combine_and_convert_ds(utils.gas_species_list, utils.aerosol_species_dict.keys(), \n",
    "                                       datasets, ['egrid', 'NEIref', 'normal_model','nonuc_model'],['egrid','NEIref','nonuc_model'], \n",
    "                                       'normal_model', 'normal_nonuc_egrid_NEIref')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f5c602e8468>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.Dataset.to_zarr(poll_ds, './data/GC_output.zarr', mode = 'w') #save the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HEMCO Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_normal = xr.open_mfdataset('../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation/OutputDir/HEMCO_diagnostics.2016*.nc', combine = 'by_coords')\n",
    "ds_nonuc = xr.open_mfdataset('../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_nonuclear/OutputDir/HEMCO_diagnostics.2016*.nc', combine = 'by_coords')\n",
    "\n",
    "ds_normal = ds_normal.isel(lev = 0)\n",
    "ds_nonuc = ds_nonuc.isel(lev = 0)\n",
    "\n",
    "ds_emis = xr.concat([ds_nonuc, ds_normal], pd.Index(['nonuc', 'normal'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_list = ['EmisNO2_Anthro', 'EmisNO_Anthro', 'EmisSO2_Anthro',  'EmisNH3_Total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_ds = ds_emis.drop_vars([species for species in ds_emis.data_vars if species not in emis_list])\n",
    "emis_ds['EmisNOx_Anthro'] = (\n",
    "        emis_ds['EmisNO2_Anthro'] + emis_ds['EmisNO_Anthro'] #sum our NO2 and NO to get NOx\n",
    "                   )\n",
    "emis_ds['EmisNOx_Anthro'].attrs['units'] = 'kg/m2/s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xarray.backends.zarr.ZarrStore at 0x7f647a315f10>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emis_ds.to_zarr('./data/emissions_HEMCO', mode = 'w') #save the dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# US-EGO Emissions Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of Plant type (in 2016 EPA egrid TSD section 5)\n",
    "The fuel category for the primary fuel of the plant. This field is “COAL” if the plant’s primary fuel is derived from coal (fuel type = BIT, COG, LIG, RC, SGC, SUB, WC), “OIL” if it is derived from oil (DFO, JF, KER, PC, RFO, WO), “GAS” if it is derived from gas (BU, NG, PG), “OFSL” if it is another fossil fuel (BFG, OG, TDF), “NUCLEAR” if it is derived from nuclear (NUC), “HYDRO” if it is derived from hydro power (WAT), “SOLAR” if it is derived from solar power, (SUN), “WIND” if it is derived from wind power (WND), “GEOTHERMAL” if it is derived from geothermal power (GEO), “OTHF” if it is derived from waste heat/unknown/purchased (MWH, OTH, PRG, PUR, WH), and “BIOMASS” if it is derived from biomass sources (AB, BLQ, LFG, MSW, OBG, OBL, OBS, SLW, WDL, WDS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import files and change hydro #####\n",
    "\n",
    "###get ORIS, plant type, and Region Name from our modified generation file\n",
    "oris_nonuc_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_no-nuclear_all-generators_20k-new_name.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "oris_normal_df = pd.read_csv('../optimization_model/good_model_inputs/inputs_gen_normal.csv',usecols=[1,2,4,5,8,22,23,24,25])\n",
    "oris_nonuc_df.loc[oris_nonuc_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "oris_normal_df.loc[oris_normal_df['FuelType'] == 'Pumps', 'FuelType'] = 'Hydro' #change pumps to hydro label\n",
    "\n",
    "###import egrid generation data and modify the index to be ORISCode\n",
    "egrid_df_raw = pd.read_excel('../raw_data/egrid2016_data.xlsx',sheet_name='GEN16', usecols='D, J, L')\n",
    "egrid = egrid_df_raw.drop(egrid_df_raw.index[0]).rename(columns={'DOE/EIA ORIS plant or facility code':'ORISCode'})\n",
    "#change those with no capacity to 1 (so that our capacity factor calculation doesn't fail)\n",
    "egrid.loc[egrid['Generator nameplate capacity (MW)'] == 0, 'Generator nameplate capacity (MW)'] = 1\n",
    "#calculate the capacity factor of generation in egrid\n",
    "egrid['egrid_capafactor'] = egrid['Generator annual net generation (MWh)'] / (8760 * egrid['Generator nameplate capacity (MW)']) \n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_normal_df=feather.read_dataframe('../optimization_model/outputs/gen_normal.feather')\n",
    "\n",
    "###import the generation data from our optimization\n",
    "gen_nonuc_df=feather.read_dataframe('../optimization_model/outputs/gen_no-nuclear_modified-all-generators.feather')\n",
    "\n",
    "### import the plant type from our egrid dataset\n",
    "planttype_df_raw=pd.read_excel('../raw_data/egrid2016_data.xlsx',sheet_name='PLNT16', usecols='D, W')\n",
    "planttype_df=planttype_df_raw.drop(egrid_df_raw.index[0]).rename(columns={'DOE/EIA ORIS plant or facility code':'ORISCode', \n",
    "                                                                       'Plant primary coal/oil/gas/ other fossil fuel category':'planttype'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### change index and convert to xarray #####\n",
    "\n",
    "#group by ORIS code and make a date and ORIS code multi index\n",
    "gen_nonuc_df = pd.concat([gen_nonuc_df,oris_nonuc_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "gen_normal_df = pd.concat([gen_normal_df,oris_normal_df['ORISCode']], axis = 1).groupby(['ORISCode']).sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_normal_df = gen_normal_df.stack()\n",
    "gen_nonuc_df = gen_nonuc_df.stack()\n",
    "gen_nonuc_df.index.names = (['ORISCode','date'])\n",
    "gen_normal_df.index.names = (['ORISCode','date'])\n",
    "\n",
    "egrid = egrid.groupby('ORISCode').sum().drop(columns = 'egrid_capafactor') #drop capacity factor because that isn't the sum\n",
    " \n",
    "planttype=planttype_df.set_index(['ORISCode'])\n",
    "planttype_sort=planttype.sort_values(by=\"ORISCode\")\n",
    "\n",
    "egrid_planttype_df = pd.concat((egrid,planttype_sort),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to xarray datasets\n",
    "oris_nonuc_ds = oris_nonuc_df.to_xarray()\n",
    "oris_normal_ds = oris_normal_df.to_xarray()\n",
    "egrid_planttype_ds = egrid_planttype_df.to_xarray()\n",
    "gen_normal_ds = gen_normal_df.to_xarray()\n",
    "gen_nonuc_ds = gen_nonuc_df.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_egrid_generation(oris_ds, gen_ds, egrid_ds):   \n",
    "    #create a capacity, fueltype, and regionname grouped by ORISCode\n",
    "    capacity = oris_ds.groupby('ORISCode').sum()['Capacity']\n",
    "    fueltype = oris_ds.to_dataframe().groupby('ORISCode').first()['FuelType']\n",
    "    regionname = oris_ds.to_dataframe().groupby('ORISCode').first()['RegionName']\n",
    "    #group by ORIS code and take the mean of everything but capacity\n",
    "    oris_ds = oris_ds.groupby('ORISCode').mean().drop('Capacity')\n",
    "    #rename our generation variable\n",
    "    gen_ds = gen_ds.rename('modelgeneration')\n",
    "\n",
    "    ###concatenate the generation and ORIS files\n",
    "    gmodel_oris_ds = xr.merge([gen_ds, oris_ds])\n",
    "    #add in the capacity\n",
    "    gmodel_oris_ds['Capacity'] = capacity\n",
    "    #create a column for capacity factors\n",
    "    gmodel_oris_ds['model_capafactor'] = 100 * gmodel_oris_ds['modelgeneration'] / (gmodel_oris_ds['Capacity'] * 8760) # % generation for each year's total capacity\n",
    "    \n",
    "    ###concatenate our model/oris and egrid emissions dataframes into one, grouped by ORIS code\n",
    "    gmodel_egrid_ds = xr.merge([gmodel_oris_ds, egrid_ds])\n",
    "    #turn all zeroes (just in the modelgeneration) to NAN\n",
    "    gmodel_egrid_ds.where('modelgeneration' == 0)['modelgeneration'] = np.nan\n",
    "    #rename the egrid data column for ease\n",
    "    gmodel_egrid_ds = gmodel_egrid_ds.rename({'Generator annual net generation (MWh)':'annual_egridgeneration'})\n",
    "    #add in fueltype\n",
    "    gmodel_egrid_ds['fueltype'] = fueltype\n",
    "    gmodel_egrid_ds = gmodel_egrid_ds.set_coords('fueltype')\n",
    "    #add in region name\n",
    "    gmodel_egrid_ds['regionname'] = regionname\n",
    "    gmodel_egrid_ds = gmodel_egrid_ds.set_coords('regionname')\n",
    "    return(gmodel_egrid_ds)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emfreese/anaconda3/envs/grid_model/lib/python3.8/site-packages/xarray/core/nanops.py:142: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis=axis, dtype=dtype)\n"
     ]
    }
   ],
   "source": [
    "#### combine ourr egrid and generation info for no nuclear and normal models ####\n",
    "nonuc_gmodel_egrid_ds = combine_egrid_generation(oris_nonuc_ds, gen_nonuc_ds, egrid_planttype_ds)\n",
    "normal_gmodel_egrid_ds = combine_egrid_generation(oris_normal_ds, gen_normal_ds, egrid_planttype_ds)\n",
    "gmodel_egrid_raw_ds = xr.concat([nonuc_gmodel_egrid_ds, normal_gmodel_egrid_ds], pd.Index(['nonuc','normal'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### fix datetime and add dec 31st at the 23rd hour ####\n",
    "\n",
    "###create a dict for dec 31st 23rd hour (just copy the dec 31 22nd hour data over)\n",
    "dec23_ds = gmodel_egrid_raw_ds.copy().isel(date = [-1])\n",
    "#change date to datetime for the 23rd hour\n",
    "dec23_ds['date'] = [datetime.datetime(2017,12,31,23,0)]\n",
    "\n",
    "###change all dates to datetime\n",
    "#make a list of dates for the year\n",
    "base = datetime.datetime(2017, 1, 1) #base date\n",
    "date_list = [base + datetime.timedelta(hours=x) for x in range(8759)] #loop through all hours but the final one (we will add it in above)\n",
    "date_list\n",
    "\n",
    "gmodel_egrid_raw_ds['date'] = date_list\n",
    "gmodel_egrid_ds = xr.merge([gmodel_egrid_raw_ds, dec23_ds])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "\n",
    "To convert our generation in MWh to get the emissions in kg/sec, we use the following (and making sure our emissions rates are in kg/sec)\n",
    "\n",
    "$MWH/3600sec -> (MW/s) * kg/MW -> kg/s$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emfreese/anaconda3/envs/grid_model/lib/python3.8/site-packages/xarray/core/common.py:671: FutureWarning: This DataArray contains multi-dimensional coordinates. In the future, the dimension order of these coordinates will be restored as well unless you specify restore_coord_dims=False.\n",
      "  return self._groupby_cls(\n"
     ]
    }
   ],
   "source": [
    "###### add NOx, SO2, CO2, CH4 emissions to gmodel_egrid dataset ######\n",
    "\n",
    "no_mult = 0.8544304 # NO/NOx as estimated from NEI2011 inventory\n",
    "no2_mult = 1 - 0.8544304 # NO2/NOx as estimated from NEI2011 inventory\n",
    "\n",
    "### process our emissions by multiplying generation* emissions factors\n",
    "gmodel_egrid_ds['model_NO_rate']  = no_mult * gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['model_NO2_rate']  = no2_mult * gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['model_SO2_rate']  =  gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLSO2RTA']\n",
    "gmodel_egrid_ds['model_CO2_rate']  =  gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLCO2RTA']\n",
    "gmodel_egrid_ds['model_CH4_rate']  =  gmodel_egrid_ds['modelgeneration']/3600 * gmodel_egrid_ds['PLCH4RTA']\n",
    "\n",
    "gmodel_egrid_ds['egrid_annual_NO_rate']  = no_mult * gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['egrid_annual_NO2_rate']  = no2_mult * gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLNOXRTA']\n",
    "gmodel_egrid_ds['egrid_annual_SO2_rate']  =  gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLSO2RTA']\n",
    "gmodel_egrid_ds['egrid_annual_CO2_rate']  =  gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLCO2RTA']\n",
    "gmodel_egrid_ds['egrid_annual_CH4_rate']  =  gmodel_egrid_ds['annual_egridgeneration']/3600 * gmodel_egrid_ds['PLCH4RTA']\n",
    "\n",
    "##### annual generation in original dataset ######\n",
    "gmodel_egrid_ds['annual_modelgeneration'] = gmodel_egrid_ds['modelgeneration'].groupby('ORISCode').sum(dim = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### create new datasets for region and fuel type grouped data, add annual generation to normal ds ######\n",
    "\n",
    "###list of pollutants\n",
    "pollutants = ['NO','NO2','SO2','CO2','CH4']\n",
    "\n",
    "def create_grouped_ds(grouper, model):\n",
    "    grouped_ds = gmodel_egrid_ds.copy().sel(model_name = model).groupby(grouper).sum()\n",
    "    grouped_ds = grouped_ds.drop(['model_capafactor','PLCH4RTA','PLCO2RTA','PLNOXRTA','PLSO2RTA','PLN2ORTA'])\n",
    "    return(grouped_ds)\n",
    "\n",
    "###create datasets of grouped fuel type and region name, combine\n",
    "nonuc_fueltype_grouped_ds = create_grouped_ds('fueltype', 'nonuc')\n",
    "normal_fueltype_grouped_ds = create_grouped_ds('fueltype', 'normal')\n",
    "\n",
    "nonuc_regionname_grouped_ds = create_grouped_ds('regionname', 'nonuc')\n",
    "normal_regionname_grouped_ds = create_grouped_ds('regionname', 'normal')\n",
    "\n",
    "fueltype_grouped_ds = xr.concat([nonuc_fueltype_grouped_ds, normal_fueltype_grouped_ds], pd.Index(['nonuc','normal'], name='model_name'))\n",
    "regionname_grouped_ds = xr.concat([nonuc_regionname_grouped_ds, normal_regionname_grouped_ds], pd.Index(['nonuc','normal'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add annual emissions of each pollutant\n",
    "for pollutant in pollutants:\n",
    "    regionname_grouped_ds[f'model_annual_{pollutant}_conc'] = (regionname_grouped_ds[f'model_{pollutant}_rate']*3600).sum(dim = 'date')\n",
    "    fueltype_grouped_ds[f'model_annual_{pollutant}_conc'] = (fueltype_grouped_ds[f'model_{pollutant}_rate']*3600).sum(dim = 'date')\n",
    "    gmodel_egrid_ds[f'model_annual_{pollutant}_conc'] = (gmodel_egrid_ds[f'model_{pollutant}_rate']*3600).sum(dim = 'date')\n",
    "\n",
    "    regionname_grouped_ds[f'egrid_annual_{pollutant}_conc'] = (regionname_grouped_ds[f'egrid_annual_{pollutant}_rate']*3600)\n",
    "    fueltype_grouped_ds[f'egrid_annual_{pollutant}_conc'] = (fueltype_grouped_ds[f'egrid_annual_{pollutant}_rate']*3600)\n",
    "    gmodel_egrid_ds[f'egrid_annual_{pollutant}_conc'] = (gmodel_egrid_ds[f'egrid_annual_{pollutant}_rate']*3600)\n",
    "\n",
    "#add ratio of difference in emissions normal-nonuc/ difference in generation normal-nonuc to the nonuc dataset\n",
    "for pollutant in pollutants:\n",
    "    fueltype_grouped_ds.sel(model_name = 'nonuc')[f'normal-nonuc_{pollutant}-gen_ratio'] = (fueltype_grouped_ds.sel(model_name = 'normal')[f'model_annual_{pollutant}_conc']-\n",
    "                                                                       fueltype_grouped_ds.sel(model_name = 'nonuc')[f'model_annual_{pollutant}_conc'])/(fueltype_grouped_ds.sel(model_name = 'normal')['annual_modelgeneration']-\n",
    "                                                                                                                             fueltype_grouped_ds.sel(model_name = 'nonuc')['annual_modelgeneration'])\n",
    "    regionname_grouped_ds.sel(model_name = 'nonuc')[f'normal-nonuc_{pollutant}-gen_ratio'] = (regionname_grouped_ds.sel(model_name = 'normal')[f'model_annual_{pollutant}_conc']-\n",
    "                                                                         regionname_grouped_ds.sel(model_name = 'nonuc')[f'model_annual_{pollutant}_conc'])/(regionname_grouped_ds.sel(model_name = 'normal')['annual_modelgeneration']-\n",
    "                                                                                                                                 regionname_grouped_ds.sel(model_name = 'nonuc')['annual_modelgeneration'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### add attributes ####\n",
    "for pollutant in pollutants:\n",
    "    if pollutant == 'NO2' or 'NO':\n",
    "                gmodel_egrid_ds[f'PLNOXRTA'].attrs['units'] = 'kg/Mwh'\n",
    "    else:\n",
    "        gmodel_egrid_ds[f'PL{pollutant}RTA'].attrs['units'] = 'kg/Mwh'\n",
    "    for idx, ds in enumerate([regionname_grouped_ds, fueltype_grouped_ds, gmodel_egrid_ds]):\n",
    "        ds[f'model_{pollutant}_rate'].attrs['units'] = 'kg/s'\n",
    "        ds[f'modelgeneration'].attrs['units'] = 'Mwh'\n",
    "        for typem in ['egrid','model']:\n",
    "            ds[f'annual_{typem}generation'].attrs['units'] = 'Mwh'\n",
    "            ds[f'{typem}_annual_{pollutant}_conc'].attrs['units'] = 'kg'\n",
    "        ds[f'egrid_annual_{pollutant}_rate'].attrs['units'] = 'kg/s'\n",
    "        ds.attrs['group'] = ['Region','Fuel Type','All'][idx]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save datasets #####\n",
    "\n",
    "### Final datasets to save are: gmodel_egrid, fueltype_grouped_ds, regionname_grouped_ds\n",
    "regionname_grouped_ds.to_netcdf(f'./data/regionname_grouped_emissions_ds.nc', 'w')\n",
    "fueltype_grouped_ds.to_netcdf(f'./data/fueltype_grouped_emissions_ds.nc', 'w')\n",
    "gmodel_egrid_ds.to_netcdf(f'./data/gmodel_egrid_emissions_ds.nc', 'w')\n",
    "\n",
    "### datasets partway to save are:\n",
    "\n",
    "oris_nonuc_ds.to_netcdf('./data/oris_nonuc_ds.nc', 'w')\n",
    "oris_normal_ds.to_netcdf('./data/oris_normal_ds.nc', 'w')\n",
    "egrid_planttype_ds.to_netcdf('./data/egrid_ds.nc', 'w')\n",
    "gen_normal_ds.to_netcdf('./data/gen_normal_ds.nc', 'w')\n",
    "gen_nonuc_ds.to_netcdf('./data/gen_nonuc_ds.nc', 'w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MERRA2 RH and T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### import MERRA2 datasets for RH and T #####\n",
    "\n",
    "#import \n",
    "T_ds = xr.open_mfdataset('../../MERRA2/2016/GEOS_0.5x0.625_NA/MERRA2/2016/*/MERRA2.2016*.A1.05x0625.NA.nc4', combine = 'by_coords')\n",
    "RH_ds = xr.open_mfdataset('../../MERRA2/2016/GEOS_0.5x0.625_NA/MERRA2/2016/*/MERRA2.2016*.A3dyn.05x0625.NA.nc4', combine = 'by_coords')\n",
    "\n",
    "#reduce datasets just to T and RH\n",
    "T_ds = T_ds['TS']\n",
    "RH_ds = RH_ds['RH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Select T and RH in the bounds of our US lat and lon #####\n",
    "T = {}\n",
    "RH = {}\n",
    "for region in utils.lat_lon_dict.keys():\n",
    "    T[region] = {}\n",
    "    T[region] = T_ds.sel(\n",
    "        lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "        lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])\n",
    "    ).groupby('time.season').mean()\n",
    "    RH[region] = {}\n",
    "    RH[region] = RH_ds.sel(\n",
    "        lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "        lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])\n",
    "    ).groupby('time.season').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Calculate seasonal mean RH and T by region #####\n",
    "T_seasonal_mean = {}\n",
    "RH_seasonal_mean = {}\n",
    "for region in utils.lat_lon_dict.keys():\n",
    "    T_seasonal_mean[region] = {}\n",
    "    RH_seasonal_mean[region] = {}\n",
    "    for seasons in ['DJF','JJA','MAM','SON']:\n",
    "        T_seasonal_mean[region][seasons] = T[region].sel(season = seasons).mean().values\n",
    "        RH_seasonal_mean[region][seasons] = RH[region].sel(season = seasons).mean().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### convert to dataframes ##### \n",
    "\n",
    "T_df = pd.DataFrame.from_dict({(i,j): T_seasonal_mean[i][j]\n",
    "                            for i in T_seasonal_mean.keys() \n",
    "                            for j in T_seasonal_mean[i].keys()},\n",
    "                            orient='index', columns = ['T'])\n",
    "T_df.index = pd.MultiIndex.from_tuples(T_df.index)\n",
    "\n",
    "RH_df = pd.DataFrame.from_dict({(i,j): RH_seasonal_mean[i][j]\n",
    "                            for i in RH_seasonal_mean.keys() \n",
    "                            for j in RH_seasonal_mean[i].keys()},\n",
    "                            orient='index', columns = ['RH'])\n",
    "RH_df.index = pd.MultiIndex.from_tuples(RH_df.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### merge dataframes #####\n",
    "RH_T_df = pd.merge(\n",
    "    T_df, RH_df, \n",
    "    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### save dataframe #####\n",
    "pd.DataFrame.to_csv(RH_T_df, './data/RH_T.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observational Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPA Observational data\n",
    "data is from https://aqs.epa.gov/aqsweb/airdata/download_files.html\n",
    "for the year 2016\n",
    "choosing O3, NO, SO2, PM25 (FEM/FRM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_obs_df = utils.import_and_edit_EPAobs('../../GEOS_CHEM/obs_data/daily*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVE Observational data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mw_dict = {'ammNO3': 80.043,\n",
    "           'ammSO4': 132.14} #g/mol \n",
    "g_ug = 1e6\n",
    "\n",
    "for species in mw_dict.keys():\n",
    "    ammon_df[f'{species}f:Value'] *= mw_dict[species] * g_ug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_PM.txt', 'PM25', 'MF')\n",
    "s_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_Sulfate.txt', 'SO4', 'SO4f')\n",
    "n_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_Nitrate.txt', 'NIT', 'NO3f')\n",
    "ammon_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_ammonia.txt', 'NH4', 'NH4f')\n",
    "oc_df = utils.import_IMPROVE('../../GEOS_CHEM/obs_data/IMPROVE_2016_OC.txt', 'OC', 'ECf')\n",
    "\n",
    "IMPROVE_df = pd.concat([pm_df, s_df, n_df,oc_df, ammon_df], axis = 0) #concatenate all dataframes and reset the index\n",
    "IMPROVE_df['Date'] = pd.to_datetime(IMPROVE_df['Date']) #change to datetime\n",
    "IMPROVE_df = IMPROVE_df.loc[IMPROVE_df['Arithmetic Mean'] >= 0] #get rid of -999 readings where there is no data\n",
    "IMPROVE_df = IMPROVE_df.loc[(IMPROVE_df['Latitude'].between(24,50,inclusive = True)) & (IMPROVE_df['Longitude'].between(-130,-60,inclusive = True))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import GC dataset\n",
    "poll_ds = xr.open_zarr('./data/GC_output.zarr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists/Dicts of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Lat and Lon of the nested grid US\n",
    "levels_dict = {'PM25':np.arange(0., 20., .5), 'SO2':np.arange(0., 5., .1), \n",
    "               'NO2':np.arange(0., 5., .1), 'NOx':np.arange(0., 5., .1), 'O3':np.arange(0., 70., 1.),\n",
    "               'dif':np.arange(-.3, .31, .01), 'regional_dif':np.arange(-1.5, 1.51, .01)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate data to get GC run data at observational points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EPA Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolate data for EPA\n",
    "interp_EPA_df = pd.DataFrame(columns=['Arithmetic Mean', 'Longitude', 'Latitude','model','species','date'])\n",
    "\n",
    "for model in poll_ds['model_name'].values:\n",
    "    for species in ['PM25', 'SO2', 'NO2', 'O3']:\n",
    "        for month in pd.DatetimeIndex(EPA_obs_df.loc[EPA_obs_df.date.dt.month.isin(utils.month_string)]['date'].values).month.unique().values:\n",
    "\n",
    "            #data selected for date\n",
    "            data = poll_ds.sel(model_name = model)[f'{species}'].groupby('time.month').mean().sel(month = month)\n",
    "            \n",
    "            #new lat and lon in radians\n",
    "            lats_new = EPA_obs_df.loc[(EPA_obs_df['species'] == species)]['Latitude'].unique()\n",
    "            lons_new = EPA_obs_df.loc[(EPA_obs_df['species'] == species)]['Longitude'].unique()\n",
    "            \n",
    "            #interpolation function\n",
    "            interp_data = []\n",
    "            for idx in range(lats_new.size):\n",
    "                interp_data.append(data.sel(lat=lats_new[idx], lon=lons_new[idx], method='nearest').values.item())\n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean':interp_data, 'Longitude':lons_new, 'Latitude':lats_new, 'model': model, 'species': species, 'date': month})\n",
    "            interp_EPA_df = interp_EPA_df.append(tmp_df, sort=False, ignore_index=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(interp_EPA_df)):\n",
    "    interp_EPA_df.loc[i,('date')] = datetime.datetime(2016,interp_EPA_df['date'][i],calendar.monthrange(2016,interp_EPA_df['date'][i])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPROVE Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolate data for EPA\n",
    "interp_IMPROVE_df = pd.DataFrame(columns=['Arithmetic Mean', 'Longitude', 'Latitude','model','species','date'])\n",
    "\n",
    "for model in poll_ds['model_name'].values:\n",
    "    for species in ['PM25','NIT','SO4','NH3','NH4','HNO3', 'TotalOC']:\n",
    "        for month in pd.DatetimeIndex(IMPROVE_df.loc[IMPROVE_df.Date.dt.month.isin(utils.month_string)]['Date'].values).month.unique().values:\n",
    "\n",
    "            #data selected for date\n",
    "            data = poll_ds.sel(model_name = model)[f'{species}'].groupby('time.month').mean().sel(month = month)\n",
    "            \n",
    "            #new lat and lon in radians\n",
    "            if species == 'NH3': #interpolating NH3 to get it for our ISORROPIA total NH4 and NH3\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NH4')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NH4')]['Longitude'].unique()\n",
    "            if species == 'HNO3': #interpolating NH3 to get it for our ISORROPIA total HNO3 and NIT\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NIT')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NIT')]['Longitude'].unique()\n",
    "            if species == 'TotalOC':\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'OC')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'OC')]['Longitude'].unique()\n",
    "            else:\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == species)]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == species)]['Longitude'].unique()\n",
    "            #interpolation function\n",
    "            interp_data = []\n",
    "            for idx in range(lats_new.size):\n",
    "                interp_data.append(data.sel(lat=lats_new[idx], lon=lons_new[idx], method='nearest').values.item())\n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean':interp_data, 'Longitude':lons_new, 'Latitude':lats_new, 'model': model, 'species': species, 'date': month})\n",
    "            interp_IMPROVE_df = interp_IMPROVE_df.append(tmp_df, sort=False, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(interp_IMPROVE_df)):\n",
    "    interp_IMPROVE_df.loc[i,('date')] = datetime.datetime(2016,interp_IMPROVE_df['date'][i],calendar.monthrange(2016,interp_IMPROVE_df['date'][i])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a monthly observational dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPROVE_df = IMPROVE_df.rename(columns = {'Date':'date'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_monthly_obs_df(obs_df):\n",
    "    \n",
    "    #create the 'geometries' for each lat and lon\n",
    "    gdf = geopandas.GeoDataFrame(\n",
    "    obs_df, geometry=geopandas.points_from_xy(obs_df.Longitude, obs_df.Latitude))\n",
    "    geometries = gdf['geometry'].apply(lambda x: x.wkt).values\n",
    "    \n",
    "    #add to the dataset\n",
    "    obs_df['geometry'] = geometries\n",
    "    obs_df.index = obs_df['date']\n",
    "    \n",
    "    #group by month and geometry\n",
    "    monthly_obs_df = pd.DataFrame(columns = ['Arithmetic Mean','Latitude','Longitude', 'geometry','species', 'date'])\n",
    "    for geometry in np.unique(geometries):\n",
    "        for species in np.unique(obs_df['species'].values):\n",
    "            lat = obs_df.loc[(obs_df['geometry'] == geometry) & (obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Latitude'].first().values\n",
    "            lon = obs_df.loc[(obs_df['geometry'] == geometry) & (obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Longitude'].first().values\n",
    "            data = obs_df.loc[(obs_df['geometry'] == geometry) & (obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Arithmetic Mean'].mean()\n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean': data.values, 'Latitude':lat, 'Longitude':lon, \n",
    "                                   'geometry':geometry, 'species': species, 'date': data.index})\n",
    "            monthly_obs_df = monthly_obs_df.append(tmp_df, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_IMPROVE_df = create_monthly_obs_df(IMPROVE_df)\n",
    "monthly_EPA_df = create_monthly_obs_df(EPA_obs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Region Names to the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-5332b760e3ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#add region to the dataframes based on lat_lon dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mIMPROVE_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_IMPROVE_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterp_EPA_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPA_obs_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonthly_EPA_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonthly_IMPROVE_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Region'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     df.loc[(df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & \n\u001b[1;32m      5\u001b[0m         (df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "#add region to the dataframes based on lat_lon dictionary\n",
    "for df in [IMPROVE_df, interp_IMPROVE_df, interp_EPA_df, EPA_obs_df, monthly_EPA_df, monthly_IMPROVE_df]:\n",
    "    df['Region'] = 'a'\n",
    "    df.loc[(df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & \n",
    "        (df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_EPA_df.to_csv('./data/interp_EPA_df.csv', date_format='%Y%m%d', index=False)\n",
    "EPA_obs_df.to_csv('./data/EPA_obs_df.csv', date_format='%Y%m%d', index=False)\n",
    "monthly_EPA_df.to_csv('./data/EPA_monthly_obs_df.csv', date_format='%Y%m%d', index=False)\n",
    "IMPROVE_df.to_csv('./data/IMPROVE_df.csv', date_format='%Y%m%d', index=False)\n",
    "interp_IMPROVE_df.to_csv('./data/interp_IMPROVE_df.csv', date_format='%Y%m%d', index=False)\n",
    "monthly_IMPROVE_df.to_csv('./data/IMPROVE_monthly_obs_df.csv', date_format='%Y%m%d', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISORROPIA Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import GC run data\n",
    "poll_ds = xr.open_zarr('./data/GC_output.zarr')\n",
    "\n",
    "#import RH and T dataframe\n",
    "RH_T_df = pd.read_csv('./data/RH_T.csv', index_col=[0,1])\n",
    "\n",
    "#subset our ds into just the species we need\n",
    "species_list = ['SO4','NH3','HNO3','NIT','NH4']\n",
    "isorropia_ds = poll_ds[species_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### species in ISORROPIA \n",
    "SO4, NH3, NO3, Cl, Ca, K, Mg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe for our isorropia monthly mean values for no nuclear and normal cases, indexed by species, location, season\n",
    "\n",
    "isorropia_dict = {}\n",
    "for species in species_list:\n",
    "    isorropia_dict[species]= {}\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "        isorropia_dict[species][region] = {}\n",
    "        for season in ['DJF','MAM','JJA','SON']:\n",
    "            data = isorropia_ds.sel(lon = slice(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1]), \n",
    "                                    lat = slice(utils.lat_lon_dict[region][2],utils.lat_lon_dict[region][3])).groupby('time.season').mean(dim = ['lat','lon','time']).sel(season = season)[species]\n",
    "            isorropia_dict[species][region][season] = data.values\n",
    "            \n",
    "isorropia_df = pd.DataFrame.from_dict({(i,j,k): isorropia_dict[i][j][k] \n",
    "                            for i in isorropia_dict.keys() \n",
    "                            for j in isorropia_dict[i].keys()\n",
    "                            for k in isorropia_dict[i][j].keys()},\n",
    "                            orient='index', columns = isorropia_ds['model_name'].values)\n",
    "\n",
    "isorropia_df.index = pd.MultiIndex.from_tuples(isorropia_df.index, names = ['Species','Location', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert from mol/mol to mol/m3 by multiplying by P/T*R (mol/m3)\n",
    "stp_p = 101325 #Pa = kg/m/s^2\n",
    "R = 8.314 #J/K/mol\n",
    "for species in species_list:\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "            for season in ['DJF','MAM','JJA','SON']:\n",
    "                isorropia_df.loc[(species,region,season)] *= stp_p/R/RH_T_df.loc[(region,season)]['T']\n",
    "                #isorropia_df.loc[(species,region,season)]['normal'] *= stp_p/R/RH_T_df.loc[(region,season)]['T']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert NIT, NH4, SO4 from ug/m3 to mol/m3\n",
    "for species in utils.aerosol_species_dict.keys():\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "            for season in ['DJF','MAM','JJA','SON']:\n",
    "                isorropia_df.loc[(species,region,season)] /= (utils.aerosol_species_dict[species]*1e6)#ug/m3/(#g/mol*#ug/g)\n",
    "\n",
    "#convert NH3, HNO3 from ppbv to mol/m3\n",
    "for species in ['NH3','HNO3']:\n",
    "    for region in utils.lat_lon_dict.keys():\n",
    "            for season in ['DJF','MAM','JJA','SON']:\n",
    "                isorropia_df.loc[(species,region,season)] *= stp_p/R/RH_T_df.loc[(region,season)]['T']/1e9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the total Nitrate and ammonium by molarity\n",
    "isorropia_df_TNO3 = isorropia_df.loc['HNO3'] + isorropia_df.loc['NIT'] \n",
    "isorropia_df_TNH3 = isorropia_df.loc['NH3'] + isorropia_df.loc['NH4'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run through ISORROPIA\n",
    "### Only run once to initiate a new ISORROPIA output, otherwise, the output is already made"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "os.chdir('../../ISORROPIAIIStandalone')\n",
    "for model in ['no_nuclear', 'normal']:\n",
    "    for region in ['SE_lat_lon', 'NW_lat_lon', 'NE_lat_lon', 'MW_lat_lon', 'SW_lat_lon']:\n",
    "            for season in ['DJF','JJA']:\n",
    "                T_tmp = RH_T_df.loc[(region, season)]['T']\n",
    "                RH_tmp = RH_T_df.loc[(region, season)]['RH']\n",
    "                SO4_tmp = isorropia_df.loc['SO4',region,season][model]\n",
    "                \n",
    "                os.system(f'mkdir -p {region}_{season}_{model}/')\n",
    "                os.system(f'cp src/* {region}_{season}_{model}/')\n",
    "                \n",
    "                os.chdir(f'{region}_{season}_{model}/')\n",
    "                print(os.listdir())\n",
    "                \n",
    "                cmdprefix = 'cat ISORange.dat | sed -i '\n",
    "                cmdT = f\"-e 's/T(K):       0.0/T(K):       {T_tmp}/' \"\n",
    "                cmdRH = f\"-e 's/RH(-):      0.0/RH(-):      {RH_tmp}/' \"\n",
    "                cmdNH3 = f\"-e 's/TSO4:       0.0e-9/TSO4:       {SO4_tmp}/' \"\n",
    "                cmdoutput = f\"-e 's/Output:     NW_DJF_nonuc_ISOOutput.nc/Output:     ISOOutput_{region}_{season}_{model}_constant_SO4.nc/' \"\n",
    "                cmdsuffix = 'ISORange.dat'\n",
    "                \n",
    "                cmd = cmdprefix+cmdT+cmdRH+cmdNH3+cmdoutput+cmdsuffix\n",
    "                \n",
    "                os.system(cmd)\n",
    "                \n",
    "                os.chdir('../')\n",
    "\n",
    "os.chdir('../grid_model/ego_nonuclear_project/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### save our data out ####\n",
    "isorropia_df.to_csv('./data/ISORROPIA_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
