{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "/home/emfreese/anaconda3/envs/conda_env/lib/python3.7/site-packages/matplotlib/mpl-data/stylelib/_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import xarray as xr\n",
    "\n",
    "import regionmask\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import cartopy.feature as cfeat\n",
    "from scipy.interpolate import RectBivariateSpline\n",
    "from datetime import datetime, timedelta\n",
    "import geopandas\n",
    "\n",
    "import calendar\n",
    "\n",
    "import glob\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import utils\n",
    "\n",
    "np.seterr(invalid='ignore'); # disable a warning from matplotlib and cartopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GEOS Chem Runs\n",
    "\n",
    "Using egrid, NEI, and our model emissions inventories for NO, NO2, SO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "egrid_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation-egrid/OutputDir/'\n",
    "NEI_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gtropchem/merra2_05x0625_tropchem_na/OutputDir/'\n",
    "MODEL_path = '../../GCrundirs/type_NA-out_conc-yr_2016-gridmod_final-validation/OutputDir/'\n",
    "speciesconc_output = 'GEOSChem.SpeciesConc.2016*.nc4'\n",
    "aerosol_output = 'GEOSChem.AerosolMass.2016*.nc4'\n",
    "\n",
    "ds_egrid, ds_NEI, ds_MODEL = utils.import_GC_runs(egrid_path, NEI_path, MODEL_path, aerosol_output, speciesconc_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    ds_egrid,\n",
    "    ds_NEI,\n",
    "    ds_MODEL\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.concat(datasets, pd.Index(['egrid', 'NEI', 'MODEL'], name='model_name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_list = [ #create a list of the species we want to keep\n",
    "    'NO',\n",
    "    'NO2',\n",
    "    'SO2',\n",
    "    'O3',\n",
    "    'NOx',\n",
    "]\n",
    "species_tracers = ['SpeciesConc_' + spec for spec in species_list]\n",
    "species_tracers.append('PM25')\n",
    "ds['SpeciesConc_NOx'] = (\n",
    "        ds['SpeciesConc_NO'] + ds['SpeciesConc_NO2'] #sum our NO2 and NO to get NOx\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_ds = ds[species_tracers].rename({'SpeciesConc_' + spec: spec for spec in species_list})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert everything to ppbv\n",
    "for species in species_list:\n",
    "    poll_ds[f'{species}'] *= 1e9 #convert from mol/mol to ppbv\n",
    "    poll_ds[f'{species}'].attrs['units'] = 'ppbv'\n",
    "\n",
    "\n",
    "for species in species_list:\n",
    "    #calculate the differences for species between model and NEI\n",
    "    poll_ds[f'dif_MODEL-NEI_{species}'] = poll_ds.sel(model_name = 'MODEL')[f'{species}'] - poll_ds.sel(model_name = 'NEI')[f'{species}']\n",
    "    poll_ds[f'dif_MODEL-NEI_{species}'].attrs['units'] = 'ppbv'\n",
    "\n",
    "    #calculate the differences for species between model and egrid\n",
    "    poll_ds[f'dif_MODEL-egrid_{species}'] = poll_ds.sel(model_name = 'MODEL')[f'{species}'] - poll_ds.sel(model_name = 'egrid')[f'{species}']\n",
    "    poll_ds[f'dif_MODEL-egrid_{species}'].attrs['units'] = 'ppbv'\n",
    "\n",
    "#calculate the dif for PM between model and NEI\n",
    "poll_ds['dif_MODEL-NEI_PM25'] = poll_ds.sel(model_name = 'MODEL')['PM25']- poll_ds.sel(model_name = 'NEI')['PM25']\n",
    "poll_ds[f'dif_MODEL-NEI_PM25'].attrs['units'] = 'PM $\\mu$g m$^{-3}$'\n",
    "\n",
    "#calculate the dif for PM between model and NEI\n",
    "poll_ds['dif_MODEL-egrid_PM25'] = poll_ds.sel(model_name = 'MODEL')['PM25']- poll_ds.sel(model_name = 'egrid')['PM25']\n",
    "poll_ds[f'dif_MODEL-egrid_PM25'].attrs['units'] = 'PM $\\mu$g m$^{-3}$'\n",
    "\n",
    "poll_ds = poll_ds.isel(lev = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xr.Dataset.to_zarr(poll_ds, 'model_NEI_egrid_GC_ds.zarr') #save the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "poll_ds = xr.open_zarr('model_NEI_egrid_GC_ds.zarr/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EPA Observational data\n",
    "data is from https://aqs.epa.gov/aqsweb/airdata/download_files.html\n",
    "for the year 2016\n",
    "choosing O3, NO, SO2, PM25 (FEM/FRM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_obs_df = pd.concat(map(pd.read_csv, glob.glob('../../GEOS_CHEM/obs_data/daily*.csv')))\n",
    "EPA_obs_df['date'] = pd.to_datetime(EPA_obs_df['Date Local'])\n",
    "EPA_obs_df['date'] = EPA_obs_df['date'].dt.normalize() + timedelta(hours=12)\n",
    "EPA_obs_df['Longitude'] = np.round(EPA_obs_df['Longitude'], decimals = 8)\n",
    "EPA_obs_df['Latitude'] = np.round(EPA_obs_df['Latitude'], decimals = 8)\n",
    "\n",
    "EPA_obs_df = EPA_obs_df.drop(columns = ['State Code','County Code','Site Num','Parameter Code','POC','Datum','Sample Duration','Date Local', 'Event Type',\n",
    "       'Observation Count', 'Observation Percent','1st Max Value', '1st Max Hour', 'Address', 'County Name', 'City Name',\n",
    "       'CBSA Name', 'Date of Last Change', 'Method Name'])\n",
    "\n",
    "#only use the SO2 1 hour 2010 pollutant standard arithmetic means and drop the 3-hour 1971 arithmetic means\n",
    "EPA_obs_df = EPA_obs_df.loc[~(EPA_obs_df['Pollutant Standard'] == 'SO2 3-hour 1971')]\n",
    "EPA_obs_df = EPA_obs_df.rename(columns = {'Parameter Name':'species'})\n",
    "\n",
    "#convert Ozone to ppb\n",
    "EPA_obs_df.loc[EPA_obs_df['species'] == 'Ozone','Arithmetic Mean'] *= 1e3 #ppb\n",
    "EPA_obs_df = EPA_obs_df.loc[~(EPA_obs_df['Arithmetic Mean'] <= 0)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPROVE Observational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mw_dict = {'ammNO3': 80.043,\n",
    "           'ammSO4': 132.14} #g/mol \n",
    "g_ug = 1e6\n",
    "\n",
    "for species in mw_dict.keys():\n",
    "    ammon_df[f'{species}f:Value'] *= mw_dict[species] * g_ug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_df = pd.read_fwf('../../GEOS_CHEM/obs_data/IMPROVE_2016_PM.txt')\n",
    "pm_df['species'] = 'PM25'\n",
    "pm_df = pm_df.rename(columns = {'MF:Value':'Arithmetic Mean', 'MF:Unc': 'Uncertainty', 'MF:Unit': 'Unit'})\n",
    "s_df = pd.read_fwf('../../GEOS_CHEM/obs_data/IMPROVE_2016_Sulfate.txt')\n",
    "s_df['species'] = 'SO4'\n",
    "s_df = s_df.rename(columns = {'SO4f:Value':'Arithmetic Mean', 'SO4f:Unc': 'Uncertainty', 'SO4f:Unit': 'Unit'})\n",
    "n_df = pd.read_fwf('../../GEOS_CHEM/obs_data/IMPROVE_2016_Nitrate.txt')\n",
    "n_df['species'] = 'NIT'\n",
    "n_df = n_df.rename(columns = {'NO3f:Value':'Arithmetic Mean', 'NO3f:Unc': 'Uncertainty', 'NO3f:Unit': 'Unit'})\n",
    "ammon_df = pd.read_fwf('../../GEOS_CHEM/obs_data/IMPROVE_2016_ammonia.txt')\n",
    "ammon_df['Arithmetic Mean'] = ammon_df['ammNO3f:Value'] + ammon_df['ammSO4f:Value']\n",
    "ammon_df = ammon_df.rename(columns = {'ammNO3f:Unit': 'Unit'})\n",
    "ammon_df = ammon_df.drop(columns = {'NH4f:Value','NH4f:Unc','NH4f:Unit','ammNO3f:Value','ammNO3f:Unc','ammSO4f:Value', 'ammSO4f:Unit', 'ammSO4f:Unc'})\n",
    "ammon_df['species'] = 'NH4'\n",
    "oc_df = pd.read_fwf('../../GEOS_CHEM/obs_data/IMPROVE_2016_OC.txt')\n",
    "oc_df['Arithmetic Mean'] = oc_df['ECf:Value'] + oc_df['OCf:Value']\n",
    "oc_df['Unit'] = oc_df['OCf:Unit']\n",
    "oc_df['species'] = 'OC_EC'\n",
    "oc_df = oc_df.drop(columns = {'ECf:Value','ECf:Unit','OCf:Value','OCf:Unit'})\n",
    "IMPROVE_df = pd.concat([pm_df, s_df, n_df,oc_df, ammon_df], axis = 0) #concatenate all dataframes and reset the index\n",
    "IMPROVE_df['Date'] = pd.to_datetime(IMPROVE_df['Date']) #change to datetime\n",
    "IMPROVE_df = IMPROVE_df.loc[IMPROVE_df['Arithmetic Mean'] >= 0] #get rid of -999 readings where there is no data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lists/Dicts of Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define Lat and Lon of the nested grid US\n",
    "levels_dict = {'PM25':np.arange(0., 20., .5), 'SO2':np.arange(0., 5., .1), \n",
    "               'NO2':np.arange(0., 5., .1), 'NOx':np.arange(0., 5., .1), 'O3':np.arange(0., 70., 1.),\n",
    "               'dif':np.arange(-.3, .31, .01), 'regional_dif':np.arange(-1.5, 1.51, .01)}\n",
    "\n",
    "\n",
    "#model_names = []\n",
    "#for d in datasets:    \n",
    "    #model_names.append(d.attrs['name'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats on Observations vs. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate data to get GC run data at observational points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EPA Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolate data for EPA\n",
    "interp_EPA_df = pd.DataFrame(columns=['Arithmetic Mean', 'Longitude', 'Latitude','model','species','date'])\n",
    "\n",
    "for model in model_names:\n",
    "    for species in list(utils.species_dict.keys()):\n",
    "        for month in pd.DatetimeIndex(EPA_obs_df.loc[EPA_obs_df.date.dt.month.isin(utils.month_string)]['date'].values).month.unique().values:\n",
    "\n",
    "            #data selected for date\n",
    "            data = poll_ds.sel(model_name = model)[f'{species}'].groupby('time.month').mean().sel(month = month)\n",
    "            \n",
    "            #new lat and lon in radians\n",
    "            lats_new = EPA_obs_df.loc[(EPA_obs_df['species'] == utils.species_dict[species])]['Latitude'].unique()\n",
    "            lons_new = EPA_obs_df.loc[(EPA_obs_df['species'] == utils.species_dict[species])]['Longitude'].unique()\n",
    "            \n",
    "            #interpolation function\n",
    "            #interp_spline= RectBivariateSpline(lats, lons, data)\n",
    "            #interp_data = interp_spline(lats_new,lons_new,grid = False)\n",
    "            interp_data = []\n",
    "            for idx in range(lats_new.size):\n",
    "                interp_data.append(data.sel(lat=lats_new[idx], lon=lons_new[idx], method='nearest').values.item())\n",
    "            \n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean':interp_data, 'Longitude':lons_new, 'Latitude':lats_new, 'model': model, 'species': utils.species_dict[species], 'date': month})\n",
    "            interp_EPA_df = interp_EPA_df.append(tmp_df, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(interp_EPA_df)):\n",
    "    interp_EPA_df.loc[i,('date')] = datetime(2016,interp_EPA_df['date'][i],calendar.monthrange(2016,interp_EPA_df['date'][i])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPROVE Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import dataset with the SO4 and NIT contributions from the model\n",
    "poll_2_ds = xr.open_zarr('pm_ds.zarr')\n",
    "#convert to ug/m3 from mol/mol\n",
    "mw_species = {\n",
    "    'NH4': 18.,\n",
    "    'NIT': 62.,\n",
    "    'SO4': 96.,\n",
    "    'BCPI': 12.,\n",
    "    'OCPI': 12.,\n",
    "    'BCPO': 12.,\n",
    "    'OCPO': 12.,\n",
    "    'DST1': 29.,\n",
    "    'DST2': 29.,\n",
    "    'SALA': 31.4,\n",
    "}\n",
    "#list of pm species\n",
    "pm_species = list(mw_species.keys())\n",
    "\n",
    "poll_2_ds = utils.ppb_to_ug(poll_2_ds, pm_species, mw_species)\n",
    "    \n",
    "poll_ds = xr.merge([poll_ds, poll_2_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = ['egrid','NEI','MODEL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['PM25', 'SO4', 'NIT', 'OC_EC', 'NH4'], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IMPROVE_df['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "### interpolate data for EPA\n",
    "interp_IMPROVE_df = pd.DataFrame(columns=['Arithmetic Mean', 'Longitude', 'Latitude','model','species','date'])\n",
    "\n",
    "for model in model_names:\n",
    "    for species in ['PM25','NIT','SO4','NH3','NH4','HNO3']:\n",
    "        for month in pd.DatetimeIndex(IMPROVE_df.loc[IMPROVE_df.Date.dt.month.isin(utils.month_string)]['Date'].values).month.unique().values:\n",
    "\n",
    "            #data selected for date\n",
    "            data = poll_ds.sel(model_name = model)[f'{species}'].groupby('time.month').mean().sel(month = month)\n",
    "            \n",
    "            #new lat and lon in radians\n",
    "            lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == species)]['Latitude'].unique()\n",
    "            lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == species)]['Longitude'].unique()\n",
    "            if species == 'NH3':\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NH4')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NH4')]['Longitude'].unique()\n",
    "            if species == 'HNO3':\n",
    "                lats_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NIT')]['Latitude'].unique()\n",
    "                lons_new = IMPROVE_df.loc[(IMPROVE_df['species'] == 'NIT')]['Longitude'].unique()\n",
    "            #interpolation function\n",
    "            interp_data = []\n",
    "            for idx in range(lats_new.size):\n",
    "                interp_data.append(data.sel(lat=lats_new[idx], lon=lons_new[idx], method='nearest').values.item())\n",
    "            \n",
    "            tmp_df = pd.DataFrame({'Arithmetic Mean':interp_data, 'Longitude':lons_new, 'Latitude':lats_new, 'model': model, 'species': species, 'date': month})\n",
    "            interp_IMPROVE_df = interp_IMPROVE_df.append(tmp_df, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(interp_IMPROVE_df)):\n",
    "    interp_IMPROVE_df.loc[i,('date')] = datetime(2016,interp_IMPROVE_df['date'][i],calendar.monthrange(2016,interp_IMPROVE_df['date'][i])[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a monthly observational dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    EPA_obs_df, geometry=geopandas.points_from_xy(EPA_obs_df.Longitude, EPA_obs_df.Latitude))\n",
    "\n",
    "geometries = gdf['geometry'].apply(lambda x: x.wkt).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPA_obs_df['geometry'] = geometries\n",
    "EPA_obs_df.index = EPA_obs_df['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_EPA_df = pd.DataFrame(columns = ['Arithmetic Mean','latitude','longitude', 'geometry','species', 'date'])\n",
    "for geometry in np.unique(geometries):\n",
    "    for species in np.unique(EPA_obs_df['species'].values):\n",
    "        lat = EPA_obs_df.loc[(EPA_obs_df['geometry'] == geometry) & (EPA_obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Latitude'].first().values\n",
    "        lon = EPA_obs_df.loc[(EPA_obs_df['geometry'] == geometry) & (EPA_obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Longitude'].first().values\n",
    "        data = EPA_obs_df.loc[(EPA_obs_df['geometry'] == geometry) & (EPA_obs_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Arithmetic Mean'].mean()\n",
    "        tmp_df = pd.DataFrame({'Arithmetic Mean': data.values, 'latitude':lat, 'longitude':lon, \n",
    "                               'geometry':geometry, 'species': species, 'date': data.index})\n",
    "        monthly_EPA_df = monthly_EPA_df.append(tmp_df, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IMPROVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geopandas.GeoDataFrame(\n",
    "    IMPROVE_df, geometry=geopandas.points_from_xy(IMPROVE_df.Longitude, IMPROVE_df.Latitude))\n",
    "\n",
    "geometries = gdf['geometry'].apply(lambda x: x.wkt).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMPROVE_df['geometry'] = geometries\n",
    "IMPROVE_df.index = IMPROVE_df['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_IMPROVE_df = pd.DataFrame(columns = ['Arithmetic Mean','latitude','longitude', 'geometry','species', 'date'])\n",
    "for geometry in np.unique(geometries):\n",
    "    for species in np.unique(IMPROVE_df['species'].values):\n",
    "        lat = IMPROVE_df.loc[(IMPROVE_df['geometry'] == geometry) & (IMPROVE_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Latitude'].first().values\n",
    "        lon = IMPROVE_df.loc[(IMPROVE_df['geometry'] == geometry) & (IMPROVE_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Longitude'].first().values\n",
    "        data = IMPROVE_df.loc[(IMPROVE_df['geometry'] == geometry) & (IMPROVE_df['species'] == species)].groupby(pd.Grouper(freq='M'))['Arithmetic Mean'].mean()\n",
    "        tmp_df = pd.DataFrame({'Arithmetic Mean': data.values, 'latitude':lat, 'longitude':lon, \n",
    "                               'geometry':geometry, 'species': species, 'date': data.index})\n",
    "        monthly_IMPROVE_df = monthly_IMPROVE_df.append(tmp_df, sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Region Names to the dataframes and save out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add region to the dataframes based on lat_lon dictionary\n",
    "\n",
    "IMPROVE_df['Region'] = 'a'\n",
    "interp_IMPROVE_df['Region'] = 'a'\n",
    "#interp_EPA_df['Region'] = 'a'\n",
    "#EPA_obs_df['Region'] = 'a'\n",
    "#monthly_EPA_df['Region'] = 'a'\n",
    "monthly_IMPROVE_df['Region'] = 'a'\n",
    "\n",
    "for region in ['SE_lat_lon','NW_lat_lon','NE_lat_lon','MW_lat_lon','SW_lat_lon']: \n",
    "\n",
    "    IMPROVE_df.loc[\n",
    "            (IMPROVE_df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & (IMPROVE_df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n",
    "    interp_IMPROVE_df.loc[\n",
    "            (interp_IMPROVE_df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & (interp_IMPROVE_df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n",
    "\n",
    "    EPA_obs_df.loc[\n",
    "        (EPA_obs_df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & (EPA_obs_df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n",
    "    interp_df.loc[\n",
    "        (interp_df['Longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & (interp_df['Latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n",
    "    monthly_EPA_df.loc[\n",
    "        (monthly_EPA_df['longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & (monthly_EPA_df['latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n",
    "\n",
    "    monthly_IMPROVE_df.loc[\n",
    "            (monthly_IMPROVE_df['longitude'].between(utils.lat_lon_dict[region][0], utils.lat_lon_dict[region][1], inclusive = True)) & (monthly_IMPROVE_df['latitude'].between(utils.lat_lon_dict[region][2], utils.lat_lon_dict[region][3], inclusive = True)), 'Region'] = region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#interp_EPA_df.to_csv('interp_EPA_df.csv', date_format='%Y%m%d', index=False)\n",
    "#EPA_obs_df.to_csv('EPA_obs_df.csv', date_format='%Y%m%d', index=False)\n",
    "#monthly_EPA_df.to_csv('EPA_monthly_obs_df.csv', date_format='%Y%m%d', index=False)\n",
    "IMPROVE_df.to_csv('IMPROVE_df.csv', date_format='%Y%m%d', index=False)\n",
    "interp_IMPROVE_df.to_csv('interp_IMPROVE_df.csv', date_format='%Y%m%d', index=False)\n",
    "monthly_IMPROVE_df.to_csv('IMPROVE_monthly_obs_df.csv', date_format='%Y%m%d', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
